---
title: "trumpafter"
author: '" "'
date: '2023-03-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph) 
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
data("stop_words")
load("C:/Users/cecei/Desktop/comp465/trumparticlesafter.RData")
mystopwords <- tibble(word = c(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here's", "sanders", "joseph", "it's", "here's", "jr", "vice")))
```

```{r}
trump <- trumparticlesafter %>% 
  unnest(headline) %>% 
  select(abstract, snippet, lead_paragraph, main, pub_date, section_name)

tidy_trump_main <- trump %>% 
  unnest_tokens(word, main) %>% 
  anti_join(stop_words) %>% 
  anti_join(mystopwords) %>% 
  count(word, sort = TRUE)

tidy_trump_main %>% 
  inner_join(get_sentiments("bing")) %>% 
  group_by(sentiment) %>% 
  slice_max(n, n = 10) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```
```{r}
tidy_trump_main %>%
  with(wordcloud(word, n, max.words = 50))
```
```{r}
trump_mix <- tibble(word = paste(trumparticlesafter$abstract, 
                                 trumparticlesafter$snippet, 
                                 trumparticlesafter$lead_paragraph, 
                                 trumparticlesafter$headline$main))

tidy_trump_mix <- trump_mix %>% 
  unnest_tokens(word, word) %>% 
  anti_join(stop_words) %>% 
  anti_join(mystopwords) %>% 
  count(word, sort = TRUE)

tidy_trump_mix %>% 
  inner_join(get_sentiments("bing")) %>% 
  group_by(sentiment) %>% 
  slice_max(n, n = 10) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)

tidy_trump_mix %>%
  with(wordcloud(word, n, max.words = 50))
```

```{r}
section_words <- tibble(word = paste(trumparticlesafter$abstract, 
                                     trumparticlesafter$snippet, 
                                     trumparticlesafter$lead_paragraph, 
                                     trumparticlesafter$headline$main),
                        section = trumparticlesafter$section_name) %>% 
  unnest_tokens(word, word) %>% 
  count(section, word, sort = TRUE)

tot_words <- section_words %>% 
  group_by(section) %>% 
  summarize(total = sum(n))

section_words <- left_join(section_words, tot_words, by = "section")

section_tf_idf <- section_words %>% 
  bind_tf_idf(word, section, n) %>% 
  arrange(desc(tf_idf))

section_tf_idf %>% 
  filter(section %in% c("U.S.", "Opinion", "World", "Business Day", "Podcasts")) %>% 
  group_by(section) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>% 
  ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = section)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~section, ncol = 2, scales = "free") +
  labs(x = "tf-idf")
```
```{r}
trump_bigrams <- tibble(word = paste(trumparticlesafter$abstract, 
                                     trumparticlesafter$snippet, 
                                     trumparticlesafter$lead_paragraph, 
                                     trumparticlesafter$headline$main),
                        section = trumparticlesafter$section_name) %>% 
  unnest_tokens(bigram, word, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram))

trump_bigram_sep <- trump_bigrams %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

section_bigram_tf_idf <- trump_bigram_sep %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word1 %in% mystopwords$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word2 %in% mystopwords$word) %>% 
  unite(bigram, word1, word2, sep = " ") %>% 
  count(section, bigram, sort = TRUE)

section_bigram_tf_idf %>% 
  bind_tf_idf(bigram, section, n) %>% 
  arrange(desc(tf_idf)) %>% 
  filter(section %in% c("U.S.", "Opinion", "World", "Business Day", "Podcasts")) %>% 
  group_by(section) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>% 
  ggplot(aes(x = tf_idf, y = fct_reorder(bigram, tf_idf), fill = section)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~section, ncol = 2, scales = "free") +
  labs(x = "tf-idf")

bigram_graph <- trump_bigram_sep %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word1 %in% mystopwords$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word2 %in% mystopwords$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 70) %>% 
  graph_from_data_frame()
```
```{r}
trump_trigrams <- tibble(word = paste(trumparticlesafter$abstract, 
                                      trumparticlesafter$snippet, 
                                      trumparticlesafter$lead_paragraph, 
                                      trumparticlesafter$headline$main),
                        section = trumparticlesafter$section_name) %>% 
  unnest_tokens(trigram, word, token = "ngrams", n = 3) %>% 
  filter(!is.na(trigram))

trump_trigram_sep <- trump_trigrams %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trump_trigram_sep %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word1 %in% mystopwords$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word2 %in% mystopwords$word) %>% 
  filter(!word3 %in% stop_words$word) %>% 
  filter(!word3 %in% mystopwords$word) %>% 
  unite(trigram, word1, word2, word3, sep = " ")
```

