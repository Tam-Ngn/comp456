---
title: "Text Mining & Sentiment Analysis"
author: '" "'
output:
  bookdown::html_document2:
    split_by: none
    toc: yes
    toc_depth: 3
    toc_float:
      toc_collapsed: true
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
library(tidyverse)
library(ggplot2)
library(dbplyr)
library(gutenbergr)
library(tidytext)
library(stringr)
library(janeaustenr)
data("stop_words")
library(scales)
library(textdata)
library(wordcloud)
```


## Background and Resources -- Cecelia
-- tidy text analysis (why do we want to do this?)
-- sentiment analysis 
-- the package (gutenbergr and what is it)
-- popular functions (and what they do)
-- resources to extend learning
- different lexicons
-- reminder about joins

## Resources
[Textbook on tidy text]<https://www.tidytextmining.com/index.html>
[Sentiment Analysis and Tidy Tuesday]<https://juliasilge.com/blog/animal-crossing/>

## The tidy text format

We define the tidy text format as being a table with one-token-per-row. This means that:
  - Each variable is a column
  - Each observation is a row
  - Each type of observation unit is a table
Therefore, a token is a meaningful unit of text, like a word, that we as data scientists are interested in analyzing. 
For tidy text mining, we may want to do a process called tokenization which splits words into tokens and then allows us to normally analyze by word. 

## Accessing the Jane Austin Books
```{r}
original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) %>%
  ungroup()

original_books
```

Now, to work with the tidy dataset we just created, we need to restructure it into a one-token-per-row format which leads us to our unnest_tokens function

```{r}
tidy_books <- original_books %>%
  unnest_tokens(word, text)
```

The unnest_tokens uses the tokenizers package to separate each line of text in the original data frame into tokens. (More on different tyoes of tokenizing later)


Now that our data is in a one-word-per-row format, we can use tidy tools (like dplyr). 

## Removing Words
We can use the tidytext dataset stop_words with an anti_join to remove common English words like "the", "of", and "to" which potentially not be fruitful in a sentiment analysis context. 

```{r}
tidy_books <- tidy_books %>%
  anti_join(stop_words)
```

## Practice
1. Find the most common words in all the tidy_books books as a whole. Create a visualization via ggplot in how you think best fits the data you get. 
```{r}
tidy_books

```


# The gutenbergr package

Another package we will be using for our sentiment analysis is the gutenbergr package, which can give us access to public domain works in the Project Gutenberg <https://www.gutenberg.org/> Collection. This is a huge package that gives us access to a large number of books and metadata around the books.


Let's look at some of the Bronte sisters' works. 

## Practice
How would we clean this dataset to prep it for sentiment analysis (think unnest_tokens and anti_join)? From there, how would we find the the most common words in the novels?

```{r}
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
```

## Section Practice

So, how do we think we can calculate the frequency of each word for the works of Jane Austin and the Bronte sisters? How would we graph this? 

```{r}
frequency <- bind_rows(mutate (bronte, author = 
"Bronte Sister"),
mutate(tidy_books, author = "Jane Austen"))
 ## Can you find a way to use a regx here?
 
```


How would we plot this?
```{r}

```


We can also run correlation tests, which allows us to quantify how similar and different these sets of word frequencies are.

Let's run a Pearson's correlation test between the Bronte sisters and Jane Austins' works. 

```{r}
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)
```

## Practice

What does this information tell you?

>>>> ANSWER:


Sentiment Analysis with tidy data

So what is sentiment analysis? Sentiment Analysis allows us to analyze the emotion in text programmatically. one of the more common ways to do this is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. 

How are sentiment lexicons created and validated? They are constructed either via crowdsourcing or by an individual which they was validated using crowdsourcing, restaurant or movie reviews, or Twitter data. 

There are a few different lexicon databases that can be used to do sentiment analysis (read more here <>) but for this we will use the nrc lexicon which categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 

```{r}
get_sentiments("nrc")
```
***NOTE: THIS WILL TAKE A WHILE TO DOWNLOAD***

Let's try an example: What are the most common joy words in the book Emma?

```{r}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

```


# Practice: How many positive and negative words are in each of the sections of the book? Here is some starter code to help you out!

```{r}
jane_austen_posneg <- tidy_books %>%
  inner_join(get_sentiments("nrc")) %>%
  count(book, ______, %/% 80,
       # We are using 80 just because of the text 
        sentiment) %>%
  # pivot_wider into sentiment and get values from the count (n)
%>% 
  mutate(sentiment = positive - negative)
```

Now try and plot the results!

```{r}

```


Practice: What are the most common positive and negative words? Us the nrc database and tidy_books. We will want to use an inner_join and a count(). 

```{r}

```


Now, make a graph with this information

```{r}

```


One cool thing with sentiment analysis is we customize our lists, like for example the word "miss" is coded as negative but can also be used as a title for a young, unmarried women in Jane Austin's workds. We can use bind_rows() to solve this:

```{r}
figure_custom_words <- bind_rows(tibble(word = c("miss"), lexicon = c("custom")),
                                 stop_words)
```


And we can make WORDCLOUDS:
```{r}
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```



## Demo of popular function
(unest_tokens(), joins for the tibbles)
- ok your turn!! --> Decide which book or books!
-- testing correlation
-- maybe give an optional challenge problem from the later chapters?

Everything we have taught into one activity


