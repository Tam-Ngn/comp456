---
title: "Text Mining & Sentiment Analysis"
author: '" "'
output:
  bookdown::html_document2:
    split_by: none
    toc: yes
    toc_depth: 3
    toc_float:
      toc_collapsed: true
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
library(tidyverse)
library(ggplot2)
library(dbplyr)
library(gutenbergr)
library(tidytext)
library(stringr)
```


## Background and Resources -- Cecelia
-- tidy text analysis (why do we want to do this?)
-- sentiment analysis 
-- the package (gutenbergr and what is it)
-- popular functions (and what they do)
-- resources to extend learning
- different lexicons
-- reminder about joins

## Resources
[Textbook on tidy text]<https://www.tidytextmining.com/index.html>
[Sentiment Analysis and Tidy Tuesday]<https://juliasilge.com/blog/animal-crossing/>

## The tidy text format

We define the tidy text format as being a table with one-token-per-row. This means that:
  - Each variable is a column
  - Each observation is a row
  - Each type of observation unit is a table
Therefore, a token is a meaningful unit of text, like a word, that we as data scientists are interested in analyzing. 
For tidy text mining, we may want to do a process called tokenization which splits words into tokens and then allows us to normally analyze by word. 

The unnest_tokens function:





## Practice
## Demo of popular function
(unest_tokens(), joins for the tibbles)
- ok your turn!! --> Decide which book or books!
-- testing correlation
-- maybe give an optional challenge problem from the later chapters?

Everything we have taught into one activity


