library(jsonlite)
library(tidyverse)
library(lubridate)
key <- "&api-key=9ta4cAbFQANV7eEtSnxC66sXFtDNHF3W"
url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump&being_date=20210101&end_date=20220101&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
link = "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump"
# url <- param_set(link, "api-key", url_encode(key))
dates <- ymd('20210101') + 0:500
d <- format(dates,'%Y%m%d')
trumparticlesafter <- NULL
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
trumparticlesafter <- bind_rows(trumparticlesafter,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(6)
}
}
save(trumparticlesafter, file = "trumparticlesafter.RData")
View(trumparticlesafter)
save(trumparticlesafter, file = "trumparticlesafter.RData")
library(jsonlite)
library(tidyverse)
library(lubridate)
key <- "&api-key=FGjAGlW0uA5Ch2aODV0Iux6uUMyxUWG0"
url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=presidentalelection&being_date=20210101&end_date=20220101&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
link = "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=presidentalelection"
# url <- param_set(link, "api-key", url_encode(key))
dates <- ymd('20210101') + 0:500
d <- format(dates,'%Y%m%d')
presidentalelectionarticlesafter <- NULL
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
presidentalelectionarticlesafter <- bind_rows(presidentalelectionarticlesafter,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(6)
}
}
save(presidentalelectionarticlesafter, file = "presidentalelectionarticlesafter.RData")
View(presidentalelectionarticlesafter)
View(req)
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
data("stop_words")
load("C:/Users/cecei/Desktop/comp465/bidenarticlesafter")
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
data("stop_words")
load("C:/Users/cecei/Desktop/comp465/bidenarticlesafter.RData")
mystopwords <- tibble(word = c(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here's", "sanders", "joseph", "it's", "here's", "jr", "vice")))
View(bidenarticlesafter)
biden <- bidenarticlesafter %>%
unnest(headline) %>%
select(abstract, snippet, lead_paragraph, main, pub_date, section_name)
tidy_biden_main <- biden %>%
unnest_tokens(word, main) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_biden_main %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
data("stop_words")
load("C:/Users/cecei/Desktop/comp465/bidenarticles.RData")
mystopwords <- tibble(word = c(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here's", "sanders", "joseph", "it's", "here's", "jr", "vice")))
biden <- bidenarticles %>%
unnest(headline) %>%
select(abstract, snippet, lead_paragraph, main, pub_date, section_name)
tidy_biden_main <- biden %>%
unnest_tokens(word, main) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_biden_main %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_biden_main %>%
with(wordcloud(word, n, max.words = 50))
biden_mix <- tibble(word = paste(bidenarticles$abstract,
bidenarticles$snippet,
bidenarticles$lead_paragraph,
bidenarticles$headline$main))
tidy_biden_mix <- biden_mix %>%
unnest_tokens(word, word) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_biden_mix %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_biden_mix %>%
with(wordcloud(word, n, max.words = 50))
biden_mix <- tibble(word = paste(bidenarticlesafter$abstract,
bidenarticlesafter$snippet,
bidenarticlesafter$lead_paragraph,
bidenarticlesafter$headline$main))
tidy_biden_mix <- biden_mix %>%
unnest_tokens(word, word) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_biden_mix %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_biden_mix %>%
with(wordcloud(word, n, max.words = 50))
section_words <- tibble(word = paste(bidenarticlesafter$abstract,
bidenarticlesafter$snippet,
bidenarticlesafter$lead_paragraph,
bidenarticlesafter$headline$main),
section = bidenarticlesafter$section_name) %>%
unnest_tokens(word, word) %>%
count(section, word, sort = TRUE)
tot_words <- section_words %>%
group_by(section) %>%
summarize(total = sum(n))
section_words <- left_join(section_words, tot_words, by = "section")
section_tf_idf <- section_words %>%
bind_tf_idf(word, section, n) %>%
arrange(desc(tf_idf))
section_tf_idf %>%
filter(section %in% c("U.S.", "Opinion", "World", "Business Day", "Podcasts")) %>%
group_by(section) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = section)) +
geom_col(show.legend = FALSE) +
facet_wrap(~section, ncol = 2, scales = "free") +
labs(x = "tf-idf")
View(tot_words)
biden_bigrams <- tibble(word = paste(bidenarticlesafter$abstract,
bidenarticlesafter$snippet,
bidenarticlesafter$lead_paragraph,
bidenarticlesafter$headline$main),
section = bidenarticlesafter$section_name) %>%
unnest_tokens(bigram, word, token = "ngrams", n = 2) %>%
filter(!is.na(bigram))
biden_bigram_sep <- biden_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
section_bigram_tf_idf <- biden_bigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
unite(bigram, word1, word2, sep = " ") %>%
count(section, bigram, sort = TRUE)
section_bigram_tf_idf %>%
bind_tf_idf(bigram, section, n) %>%
arrange(desc(tf_idf)) %>%
filter(section %in% c("U.S.", "Opinion", "World", "Business Day", "Podcasts")) %>%
group_by(section) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(x = tf_idf, y = fct_reorder(bigram, tf_idf), fill = section)) +
geom_col(show.legend = FALSE) +
facet_wrap(~section, ncol = 2, scales = "free") +
labs(x = "tf-idf")
bigram_graph <- biden_bigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(n > 70) %>%
graph_from_data_frame()
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 3) +
geom_node_text(aes(label = name), vjust = 0.2, hjust = 0.2) +
theme_void()
biden_trigrams <- tibble(word = paste(bidenarticlesafter$abstract,
bidenarticlesafter$snippet,
bidenarticlesafter$lead_paragraph,
bidenarticlesafter$headline$main),
section = bidenarticlesafter$section_name) %>%
unnest_tokens(trigram, word, token = "ngrams", n = 3) %>%
filter(!is.na(trigram))
biden_trigram_sep <- biden_trigrams %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ")
biden_trigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
filter(!word3 %in% stop_words$word) %>%
filter(!word3 %in% mystopwords$word) %>%
unite(trigram, word1, word2, word3, sep = " ")
View(biden_mix)
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
data("stop_words")
load("C:/Users/cecei/Desktop/comp465/trumparticlesafter.RData")
mystopwords <- tibble(word = c(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here's", "sanders", "joseph", "it's", "here's", "jr", "vice")))
trump <- trumparticlesafter %>%
unnest(headline) %>%
select(abstract, snippet, lead_paragraph, main, pub_date, section_name)
tidy_biden_main <- biden %>%
unnest_tokens(word, main) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_biden_main %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
trump <- trumparticlesafter %>%
unnest(headline) %>%
select(abstract, snippet, lead_paragraph, main, pub_date, section_name)
tidy_biden_main <- trump %>%
unnest_tokens(word, main) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_biden_main %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
trump <- trumparticlesafter %>%
unnest(headline) %>%
select(abstract, snippet, lead_paragraph, main, pub_date, section_name)
tidy_trump_main <- trump %>%
unnest_tokens(word, main) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_trump_main %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_trump_main %>%
with(wordcloud(word, n, max.words = 50))
trump_mix <- tibble(word = paste(trumparticlesafter$abstract,
trumparticlesafter$snippet,
trumparticlesafter$lead_paragraph,
trumparticlesafter$headline$main))
tidy_trump_mix <- trump_mix %>%
unnest_tokens(word, word) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_trump_mix %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_trump_mix %>%
with(wordcloud(word, n, max.words = 50))
section_words <- tibble(word = paste(trumparticlesafter$abstract,
trumparticlesafter$snippet,
trumparticlesafter$lead_paragraph,
trumparticlesafter$headline$main),
section = trumparticlesafter$section_name) %>%
unnest_tokens(word, word) %>%
count(section, word, sort = TRUE)
tot_words <- section_words %>%
group_by(section) %>%
summarize(total = sum(n))
section_words <- left_join(section_words, tot_words, by = "section")
section_tf_idf <- section_words %>%
bind_tf_idf(word, section, n) %>%
arrange(desc(tf_idf))
section_tf_idf %>%
filter(section %in% c("U.S.", "Opinion", "World", "Business Day", "Podcasts")) %>%
group_by(section) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = section)) +
geom_col(show.legend = FALSE) +
facet_wrap(~section, ncol = 2, scales = "free") +
labs(x = "tf-idf")
trump_bigrams <- tibble(word = paste(trumparticlesafter$abstract,
trumparticlesafter$snippet,
trumparticlesafter$lead_paragraph,
trumparticlesafter$headline$main),
section = trumparticlesafter$section_name) %>%
unnest_tokens(bigram, word, token = "ngrams", n = 2) %>%
filter(!is.na(bigram))
trump_bigram_sep <- trump_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
section_bigram_tf_idf <- trump_bigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
unite(bigram, word1, word2, sep = " ") %>%
count(section, bigram, sort = TRUE)
section_bigram_tf_idf %>%
bind_tf_idf(bigram, section, n) %>%
arrange(desc(tf_idf)) %>%
filter(section %in% c("U.S.", "Opinion", "World", "Business Day", "Podcasts")) %>%
group_by(section) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(x = tf_idf, y = fct_reorder(bigram, tf_idf), fill = section)) +
geom_col(show.legend = FALSE) +
facet_wrap(~section, ncol = 2, scales = "free") +
labs(x = "tf-idf")
bigram_graph <- trump_bigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(n > 70) %>%
graph_from_data_frame()
trump_trigrams <- tibble(word = paste(trumparticlesafter$abstract,
trumparticlesafter$snippet,
trumparticlesafter$lead_paragraph,
trumparticlesafter$headline$main),
section = trumparticlesafter$section_name) %>%
unnest_tokens(trigram, word, token = "ngrams", n = 3) %>%
filter(!is.na(trigram))
trump_trigram_sep <- trump_trigrams %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ")
trump_trigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
filter(!word3 %in% stop_words$word) %>%
filter(!word3 %in% mystopwords$word) %>%
unite(trigram, word1, word2, word3, sep = " ")
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
data("stop_words")
load("C:/Users/cecei/Desktop/comp465/bidenarticles.RData")
mystopwords <- tibble(word = c(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here's", "sanders", "joseph", "it's", "here's", "jr", "vice")))
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
data("stop_words")
load("C:/Users/cecei/Desktop/comp465/bidenarticles.RData")
mystopwords <- tibble(word = c(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here/'s", "sanders", "joseph", "it's", "here's", "jr", "vice")))
biden <- bidenarticles %>%
unnest(headline) %>%
select(abstract, snippet, lead_paragraph, main, pub_date, section_name)
tidy_biden_main <- biden %>%
unnest_tokens(word, main) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_biden_main %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_biden_main %>%
with(wordcloud(word, n, max.words = 50))
load("C:/Users/cecei/Desktop/comp465/bidenarticles.RData")
mystopwords <- tibble(word = c(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here/'s", "sanders", "joseph", "it/'s", "here's", "jr", "vice")))
tidy_biden_main %>%
with(wordcloud(word, n, max.words = 50))
biden_mix <- tibble(word = paste(bidenarticles$abstract,
bidenarticles$snippet,
bidenarticles$lead_paragraph,
bidenarticles$headline$main))
tidy_biden_mix <- biden_mix %>%
unnest_tokens(word, word) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_biden_mix %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_biden_mix %>%
with(wordcloud(word, n, max.words = 50))
