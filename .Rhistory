}
else if (response$status_code == 404) {
cat("The url:", web_url[i], "returned a 404 status code. Skipping to the next url. \n")
next
}
else {
cat("API request failed with status code", response$status_code, "\n")
break
}
}, error = function(e) {
cat("Error in API request:", conditionMessage(e), "\n")
break
})
}
for (i in 997:6000) {
tryCatch({
response <- GET(totalarticles$web_url[i])
if (response$status_code == 500) {
failures <- failures + 1
if (failures < 3) {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 60s and retrying \n")
Sys.sleep(60)
}
else {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 180s and retrying \n")
Sys.sleep(180)
failures <- 0
}
}
else if (response$status_code == 200) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$web_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
cat("# of article text pulled: ", nrow(body_text_tot), "\n")
}
else if (response$status_code == 404) {
cat("The url:", web_url[i], "returned a 404 status code. Skipping to the next url. \n")
next
}
else {
cat("API request failed with status code", response$status_code, "\n")
break
}
}, error = function(e) {
cat("Error in API request:", conditionMessage(e), "\n")
break
})
}
for (i in 1234:6000) {
tryCatch({
response <- GET(totalarticles$web_url[i])
if (response$status_code == 500) {
failures <- failures + 1
if (failures < 3) {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 60s and retrying \n")
Sys.sleep(60)
}
else {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 180s and retrying \n")
Sys.sleep(180)
failures <- 0
}
}
else if (response$status_code == 200) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$web_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
cat("# of article text pulled: ", nrow(body_text_tot), "\n")
}
else if (response$status_code == 404) {
cat("The url:", web_url[i], "returned a 404 status code. Skipping to the next url. \n")
next
}
else {
cat("API request failed with status code", response$status_code, "\n")
break
}
}, error = function(e) {
cat("Error in API request:", conditionMessage(e), "\n")
break
})
}
for (i in 1765:6000) {
tryCatch({
response <- GET(totalarticles$web_url[i])
if (response$status_code == 500) {
failures <- failures + 1
if (failures < 3) {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 60s and retrying \n")
Sys.sleep(60)
}
else {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 180s and retrying \n")
Sys.sleep(180)
failures <- 0
}
}
else if (response$status_code == 200) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$web_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
cat("# of article text pulled: ", nrow(body_text_tot), "\n")
}
else if (response$status_code == 404) {
cat("The url:", web_url[i], "returned a 404 status code. Skipping to the next url. \n")
next
}
else {
cat("API request failed with status code", response$status_code, "\n")
break
}
}, error = function(e) {
cat("Error in API request:", conditionMessage(e), "\n")
break
})
}
for (i in 3055:6000) {
tryCatch({
response <- GET(totalarticles$web_url[i])
if (response$status_code == 500) {
failures <- failures + 1
if (failures < 3) {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 60s and retrying \n")
Sys.sleep(60)
}
else {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 180s and retrying \n")
Sys.sleep(180)
failures <- 0
}
}
else if (response$status_code == 200) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$web_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
cat("# of article text pulled: ", nrow(body_text_tot), "\n")
}
else if (response$status_code == 404) {
cat("The url:", web_url[i], "returned a 404 status code. Skipping to the next url. \n")
next
}
else {
cat("API request failed with status code", response$status_code, "\n")
break
}
}, error = function(e) {
cat("Error in API request:", conditionMessage(e), "\n")
break
})
}
for (i in 4483:6000) {
tryCatch({
response <- GET(totalarticles$web_url[i])
if (response$status_code == 500) {
failures <- failures + 1
if (failures < 3) {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 60s and retrying \n")
Sys.sleep(60)
}
else {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 180s and retrying \n")
Sys.sleep(180)
failures <- 0
}
}
else if (response$status_code == 200) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$web_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
cat("# of article text pulled: ", nrow(body_text_tot), "\n")
}
else if (response$status_code == 404) {
cat("The url:", web_url[i], "returned a 404 status code. Skipping to the next url. \n")
next
}
else {
cat("API request failed with status code", response$status_code, "\n")
break
}
}, error = function(e) {
cat("Error in API request:", conditionMessage(e), "\n")
break
})
}
for (i in 4902:6000) {
tryCatch({
response <- GET(totalarticles$web_url[i])
if (response$status_code == 500) {
failures <- failures + 1
if (failures < 3) {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 60s and retrying \n")
Sys.sleep(60)
}
else {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 180s and retrying \n")
Sys.sleep(180)
failures <- 0
}
}
else if (response$status_code == 200) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$web_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
cat("# of article text pulled: ", nrow(body_text_tot), "\n")
}
else if (response$status_code == 404) {
cat("The url:", web_url[i], "returned a 404 status code. Skipping to the next url. \n")
next
}
else {
cat("API request failed with status code", response$status_code, "\n")
break
}
}, error = function(e) {
cat("Error in API request:", conditionMessage(e), "\n")
break
})
}
for (i in 5427:6000) {
tryCatch({
response <- GET(totalarticles$web_url[i])
if (response$status_code == 500) {
failures <- failures + 1
if (failures < 3) {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 60s and retrying \n")
Sys.sleep(60)
}
else {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 180s and retrying \n")
Sys.sleep(180)
failures <- 0
}
}
else if (response$status_code == 200) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$web_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
cat("# of article text pulled: ", nrow(body_text_tot), "\n")
}
else if (response$status_code == 404) {
cat("The url:", web_url[i], "returned a 404 status code. Skipping to the next url. \n")
next
}
else {
cat("API request failed with status code", response$status_code, "\n")
break
}
}, error = function(e) {
cat("Error in API request:", conditionMessage(e), "\n")
break
})
}
for (i in 5610:6000) {
tryCatch({
response <- GET(totalarticles$web_url[i])
if (response$status_code == 500) {
failures <- failures + 1
if (failures < 3) {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 60s and retrying \n")
Sys.sleep(60)
}
else {
cat("# of failures :", failures, "\n", "API request failed with 500 status code. Pause 180s and retrying \n")
Sys.sleep(180)
failures <- 0
}
}
else if (response$status_code == 200) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$web_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
cat("# of article text pulled: ", nrow(body_text_tot), "\n")
}
else if (response$status_code == 404) {
cat("The url:", web_url[i], "returned a 404 status code. Skipping to the next url. \n")
next
}
else {
cat("API request failed with status code", response$status_code, "\n")
break
}
}, error = function(e) {
cat("Error in API request:", conditionMessage(e), "\n")
break
})
}
save(body_text_tot, file = "biden_nytimes_2021_text_01.RData")
body_text_tot1 <- body_text_tot
load("~/Desktop/SPRING 2023/STAT 456/comp465/biden_nytimes_2021_text_2.RData")
body_text_tot <- rbind(body_text_tot1, body_text_tot)
save(body_text_tot, file = "biden_nytimes_2021_text.RData")
load("~/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian_text.RData")
View(body_text_tot)
body_text_tot$url[9]
library(keras)
install.packages("keras")
library(keras)
library(dplyr)
library(tidyr)
# Step 1:
all_data <- read.csv('https://raw.githubusercontent.com/JeremyHub/STAT-456-Final/main/Jeremy/train.csv') # characteristics of homes sold in an Iowa town between 2006 and 2010. Compiled specifically for use in data science education.
# Step 2:
train <- all_data[,sapply(all_data, is.numeric)] %>%
sapply(function(x) (x - min(x)) / (max(x) - min(x))) %>% # applies the function to perform min-max scaling
as.data.frame() %>%
select(-LotFrontage,-MasVnrArea,-GarageYrBlt) %>% # filters out all non-numeric columns as the model doesn't know how to handle strings (if you wanted it to handle categorical strings, you could map each string to a number and let the model figure out that it is categorical)
na.omit()
test <- train[0:500,] # the first 500 rows belongs to the test set
train <- train[500:nrow(train),] # the rest of the dataset belongs to the training set
all_data
x_train <- train %>%
select(-SalePrice,-Id) # keeps all columns except SalePrice and Id since they're not predictors
y_train <- train %>%
select(SalePrice) # assigns SalePrice to y as an outcome variable
x_test <- test %>%
select(-SalePrice,-Id)
y_test <- test %>%
select(SalePrice)
# reshape
x_train <- x_train %>%
unlist() %>%
array(dim = c(nrow(x_train),33))
x_test <- x_test %>%
unlist() %>%
array(dim = c(nrow(x_test),33))
y_train <- y_train %>%
unlist() %>%
array(dim=c(nrow(y_train),1))
y_test <- y_test %>%
unlist() %>%
array(dim=c(nrow(y_test),1))
# install_keras() # Uncomment this line and run it once to install the newest version of keras on your computer, then comment out this line again
model <- keras_model_sequential()
Y
setwd("~/Desktop/SPRING 2023/STAT 456/comp465")
load("~/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/Biden_Guardian_Before_FullText.RData")
View(g_b_b_fullText)
load("~/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/Biden_Guardian_After_FullText.RData")
View(g_b_a)
View(g_b_a)
View(g_b_a)
View(g_b_b_fullText)
biden_guardian_text <- rbind(g_b_a, g_b_b_fullText)
biden_guardian_2020 <- g_b_b_fullText %>%
select(url, sectionName, webPublicationDate, text)
library(rvest)
library(readxl)
library(tidyverse)
biden_guardian_2020 <- g_b_b_fullText %>%
select(url, sectionName, webPublicationDate, text)
biden_guardian_2021 <- g_b_a %>%
select(url, sectionName, webPublicationDate, text)
biden_guardian_2021 <- g_b_a %>%
select(webUrl, sectionName, webPublicationDate, text) %>%
rename(url = webUrl)
biden_guardian <- rbind(biden_guardian_2020, biden_guardian_2021)
save(biden_guardian, file = "biden_guardian_text.RData")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(ggraph)
data("stop_words")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s"))
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian.RData")
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian_text.RData")
tidy_biden_text <- biden_guardian %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(str_detect(text,'Biden')) %>%
filter(text != "")
View(tidy_biden_text)
tidy_trump_text <- body_text_tot %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(str_detect(text,'Trump')) %>%
filter(text != "")
View(tidy_trump_text)
tidy_biden_text %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date))
tidy_biden_text_time <- tidy_biden_text %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, webUrl) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score))
tidy_biden_text_time <- tidy_biden_text %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, url) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score))
View(tidy_biden_text_time)
View(tidy_biden_text)
tidy_trump_text_time <- tidy_trump_text %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text, webUrl) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, webUrl) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score))
View(tidy_trump_text_time)
View(tidy_trump_text_time)
tidy_biden_text <- biden_guardian %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(str_detect(text,'biden')) %>%
filter(text != "")
tidy_biden_text_time <- tidy_biden_text %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, url) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score))
biden_guardian
View(biden_guardian)
tidy_biden_text <- biden_guardian %>%
mutate(text = str_remove_all(text, "[:punct:]"))
tidy_biden_text <- biden_guardian %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(str_detect(text,'biden'))
tidy_biden_text <- biden_guardian %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(str_detect(text,'biden')) %>%
filter(text != "")
tidy_biden_text_time <- tidy_biden_text %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, url) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score))
View(tidy_biden_text_time)
tidy_biden_text <- biden_guardian %>%
mutate(text = str_remove_all(text, "[:punct:]"))
tidy_biden_text_time <- tidy_biden_text %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, url) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score))
View(tidy_biden_text_time)
