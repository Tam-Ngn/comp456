knitr::opts_chunk$set(echo = TRUE, error = TRUE)
library(tidyverse)
library(ggplot2)
library(dbplyr)
library(gutenbergr) #install.packages('gutenbergr')
library(tidytext)  #install.packages('tidytext')
library(stringr)
library(janeaustenr) #install.packages('janeaustenr')
data("stop_words")
library(scales)
library(textdata) #install.packages('textdata')
library(wordcloud)
library(igraph) #to create an igraph object
library(ggraph) #to visualize the igraph object by turning it into ggraph with appropriate functions
# DTM example from https://www.tidytextmining.com/dtm.html#dtm
data("AssociatedPress", package = "topicmodels")
class(AssociatedPress)
# DTM to a tidy format
ap_td <- tidy(AssociatedPress)
head(ap_td)
# tidy dataframe to DTM
ap_td %>%
cast_dtm(document, term, count) %>%
head()
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
mutate(text = str_replace_all(text,"_"," "))
original_books %>% head()
tidy_books <- original_books %>%
unnest_tokens(word, text) # we'll break the text variable into word tokens
head(tidy_books)
load("~/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/Biden_Guardian_Before_FullText.RData")
View(g_b_b_fullText)
load("~/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/Biden_Guardian_After_FullText.RData")
View(g_b_a)
setwd("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465")
load("~/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian_text.RData")
load("~/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian.RData")
knitr::opts_chunk$set(echo = TRUE)
tidy_body_text_tot <- body_text_tot %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(str_detect(text,'Trump')) %>%
filter(text != "")
View(totalarticles)
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text, webUrl) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, webUrl) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score)) %>%
ggplot(aes(x = date, y = day_score, group = 1)) +
geom_line() +
geom_hline(yintercept = 0) +
geom_vline(xintercept = mdy('11-08-2020'),color='red') +
geom_smooth(se=FALSE)+
scale_colour_brewer(palette = "Set1") +
scale_x_date(date_breaks="2 month", date_labels="%m-%Y") +
labs(title = "Guardian 2020 through 2022",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") + theme_classic()
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(ggraph)
data("stop_words")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s"))
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian.RData")
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian_text.RData")
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text, webUrl) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, webUrl) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score)) %>%
ggplot(aes(x = date, y = day_score, group = 1)) +
geom_line() +
geom_hline(yintercept = 0) +
geom_vline(xintercept = mdy('11-08-2020'),color='red') +
geom_smooth(se=FALSE)+
scale_colour_brewer(palette = "Set1") +
scale_x_date(date_breaks="2 month", date_labels="%m-%Y") +
labs(title = "Guardian 2020 through 2022",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") + theme_classic()
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text, webUrl) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, webUrl) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score)) %>%
ggplot(aes(x = date, y = day_score, group = 1)) +
geom_line() +
geom_hline(yintercept = 0) +
geom_vline(xintercept = mdy('11-08-2020'),color='red') +
geom_smooth(se=FALSE)+
scale_colour_brewer(palette = "Set1") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
scale_x_date(date_breaks="2 month", date_labels="%m-%Y") +
labs(title = "Guardian 2020 through 2022",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") + theme_classic()
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text, webUrl) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, webUrl) %>%
summarize(art_score = mean(value)) %>%
group_by(date) %>%
summarize(day_score = mean(art_score)) %>%
ggplot(aes(x = date, y = day_score, group = 1)) +
geom_line() +
geom_hline(yintercept = 0) +
geom_vline(xintercept = mdy('11-08-2020'),color='red') +
geom_smooth(se=FALSE)+
scale_colour_brewer(palette = "Set1") +
scale_x_date(date_breaks="2 month", date_labels="%m-%Y") +
labs(title = "Guardian 2020 through 2022",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") + theme_classic()
article <- NULL
body_text_tot <- NULL
for (i in 1:length(totalarticles$webUrl)) {
article <- read_html(totalarticles$webUrl[i])
body_text <-
article %>%
html_elements(".dcr-n6w1lc", ".dcr-az7egx", ".dcr-1up63on", ".dcr-8zipgp", ".dcr-94xsh", "dcr-1gesh1i") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$webUrl[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
}
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
article <- NULL
body_text_tot <- NULL
for (i in 1:10) {
article <- read_html(totalarticles$webUrl[i])
body_text <-
article %>%
html_elements(".dcr-n6w1lc", ".dcr-az7egx", ".dcr-1up63on", ".dcr-8zipgp", ".dcr-94xsh", "dcr-1gesh1i") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$webUrl[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
}
article <- NULL
body_text_tot <- NULL
for (i in 1:10) {
article <- read_html(totalarticles$webUrl[i])
body_text <-
article %>%
html_elements(".dcr-n6w1lc", ".dcr-az7egx", ".dcr-1up63on", ".dcr-8zipgp", ".dcr-94xsh", "dcr-1gesh1i") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$webUrl[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
}
body_text <-
article %>%
html_elements(".dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_text()
body_text
article
View(article)
totalarticles$webUrl[i]
article %>%
html_elements("article-body-commercial-selector article-body-viewer-selector")
article %>%
html_elements("article-body-commercial-selector, article-body-viewer-selector")
article %>%
html_elements(".article-body-viewer-selector")
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class')
?stringr
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class') %>% str_split(' ')
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class') %>% str_split(' ') %>% sapply() %>% tail(1)
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class') %>% str_split(' ') %>% sapply(tail, n=1)
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class') %>% str_split(' ') %>% sapply(tail, n=1) %>% paste0('.',.)
cssSelector <-
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class') %>% str_split(' ') %>% sapply(tail, n=1) %>% paste0('.',.)
article %>%
html_elements(cssSelector) %>%   html_text()
i=2
article <- read_html(totalarticles$webUrl[i])
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class') %>% str_split(' ') %>% sapply(tail, n=1) %>% paste0('.',.)
body_text <- article %>%
html_elements(cssSelector) %>%   html_text()
body_text
tibble(url = totalarticles$webUrl[i], text = paste(body_text, collapse = " "))
for (i in 1:10) {
article <- read_html(totalarticles$webUrl[i])
cssSelector <-
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class') %>% str_split(' ') %>% sapply(tail, n=1) %>% paste0('.',.)
body_text <- article %>%
html_elements(cssSelector) %>%   html_text()
body_text_coll<- tibble(url = totalarticles$webUrl[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
}
i
article
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class') %>% str_split(' ') %>% sapply(tail, n=1) %>% paste0('.',.)
article %>%
html_elements(".article-body-viewer-selector")
totalarticles$webUrl[i]
body_text  = NULL
body_text_coll<- tibble(url = totalarticles$webUrl[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
article <- NULL
body_text_tot <- NULL
for (i in 1:10) {
article <- read_html(totalarticles$webUrl[i])
cssSelector <-
article %>%
html_elements(".article-body-viewer-selector") %>% #.dcr-n6w1lc, .dcr-az7egx, .dcr-1up63on, .dcr-8zipgp, .dcr-94xsh, .dcr-1gesh1i") %>%
html_attr('class') %>% str_split(' ') %>% sapply(tail, n=1) %>% paste0('.',.)
if(cssSelector =='.'){body_text  = NULL}else{
body_text <- article %>%
html_elements(cssSelector) %>%   html_text()}
body_text_coll<- tibble(url = totalarticles$webUrl[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
}
body_text_tot
load("~/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian_text.RData")
View(body_text_tot)
