# save(totalarticles, file = "trump_guardian.RData")
body_text_tot <- NULL
for (i in 1:length(totalarticles$web_url)) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$webb_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
}
save(body_text_tot, file = "trump_nyt.RData")
View(body_text_tot)
View(body_text_tot)
View(body_text_coll)
View(totalarticles)
load("~/Documents/GitHub/comp456/trump_nyt.RData")
load("~/Documents/GitHub/comp456/trump_nyt.RData")
load("~/Documents/GitHub/comp456/trump_nyt.RData")
totalarticles <-  totalarticles %>%
mutate(date = str_trunc(pub_date, width = 10, ellipsis = "")) %>%
filter(date >= "2020-01-01")
load("~/Documents/GitHub/comp456/trump_guardian_2020.RData")
load("~/Documents/GitHub/comp456/Articles Data/trump_guardian.RData")
View(totalarticles)
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
library(readxl)
key <- "&api-key=CunYbsfgJWDXmpfcvKnoW1G3TBAY6grG"
url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump&begin_date=20200101&end_date=20200101&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
link = "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump"
keys <- c("&api-key=CunYbsfgJWDXmpfcvKnoW1G3TBAY6grG",
"&api-key=gDxQ32ZZfP8KarCN5MrGdmrkeKfkko7u",
"&api-key=Cg6eP60vTxQAEtZoez9YccqiF9CHCyCA",
"&api-key=9fiVSl9AtqEaHqtInQLx3V56dmUgYy27",
"&api-key=kfpVf3ML5uymHzA83Ai5op7AZfQbkjcB",
"&api-key=dfrzINL05mURMbUrnLDGgsNWBlAqVR9n")
dates <- ymd('20200101') + 0:5
d <- format(dates,'%Y%m%d')
totalarticles <- NULL
# for (i in 1:length(keys)) {
#   key = keys[i]
#   for(i in d){
#     p = 0
#     while(p < 10){
#       url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
#       req <- fromJSON(paste0(url, key))
#       articles <- req$response$docs
#       totalarticles <- bind_rows(totalarticles,articles)
#       if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
#       else{p = p+1}
#       Sys.sleep(12)
#     }
#   }
# }
pr = 0
for(i in d){
p = 0
while(p < 10){
key <- keys[pr %% length(keys)+1] #modular arithmetic
#key <- sample(keys, size = 1)
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
pr = pr + 1
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(is.null(nrow(articles)) | (isTRUE(nrow(articles)) && nrow(articles) != 10)){ break }
else{p = p+1}
Sys.sleep(2)
}
}
trumpnyttotal <-  totalarticles %>%
mutate(date = str_trunc(pub_date, width = 10, ellipsis = "")) %>%
filter(date >= "2020-01-01")
# save(trumpnyttotal, file = "trump_guardian.RData")
body_text_tot <- NULL
for (i in 1:length(totalarticles$web_url)) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$webb_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
}
save(body_text_tot, file = "trump_nyt.RData")
View(totalarticles)
View(trumpnyttotal)
View(body_text_coll)
View(body_text_tot)
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
library(readxl)
key <- "&api-key=CunYbsfgJWDXmpfcvKnoW1G3TBAY6grG"
url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump&begin_date=20200101&end_date=20200101&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
link = "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump"
keys <- c("&api-key=CunYbsfgJWDXmpfcvKnoW1G3TBAY6grG",
"&api-key=gDxQ32ZZfP8KarCN5MrGdmrkeKfkko7u",
"&api-key=Cg6eP60vTxQAEtZoez9YccqiF9CHCyCA",
"&api-key=9fiVSl9AtqEaHqtInQLx3V56dmUgYy27",
"&api-key=kfpVf3ML5uymHzA83Ai5op7AZfQbkjcB",
"&api-key=dfrzINL05mURMbUrnLDGgsNWBlAqVR9n")
dates <- ymd('20200101') + 0:5
d <- format(dates,'%Y%m%d')
totalarticles <- NULL
# for (i in 1:length(keys)) {
#   key = keys[i]
#   for(i in d){
#     p = 0
#     while(p < 10){
#       url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
#       req <- fromJSON(paste0(url, key))
#       articles <- req$response$docs
#       totalarticles <- bind_rows(totalarticles,articles)
#       if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
#       else{p = p+1}
#       Sys.sleep(12)
#     }
#   }
# }
pr = 0
for(i in d){
p = 0
while(p < 10){
key <- keys[pr %% length(keys)+1] #modular arithmetic
#key <- sample(keys, size = 1)
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
pr = pr + 1
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(is.null(nrow(articles)) | (isTRUE(nrow(articles)) && nrow(articles) != 10)){ break }
else{p = p+1}
Sys.sleep(6)
}
}
trumpnyttotal <-  totalarticles %>%
mutate(date = str_trunc(pub_date, width = 10, ellipsis = "")) %>%
filter(date >= "2020-01-01")
# save(trumpnyttotal, file = "trump_guardian.RData")
body_text_tot <- NULL
for (i in 1:length(totalarticles$web_url)) {
article <- read_html(totalarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$webb_url[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
}
save(body_text_tot, file = "trump_nyt.RData")
View(trumpnyttotal)
View(totalarticles)
=======
View(totalarticles)
totalarticles %>% count(web_url)
totalarticles %>% count(web_url) %>% filter(n>1) %>% nrow()
?cat
knitr::opts_chunk$set(echo = TRUE)
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian.RData")
body_text_tot
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian_text.RData")
body_text_tot
body_text_tot %>% filter(str_detect(text,'trump'))
body_text_tot %>% filter(str_detect(text,'trump')) %>% View()
body_text_tot %>% filter(str_detect(text,'Trump')) %>% nrow()
body_text_tot %>% filter(!str_detect(text,'Trump')) %>% nrow()
body_text_tot %>% filter(!str_detect(text,'Trump')) %>% View()
body_text_tot %>% filter(!str_detect(text,'Trump')) %>% pull(text)
>>>>>>> Stashed changes
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(ggraph)
data("stop_words")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s"))
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian.RData")
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian_text.RData")
#body_text_tot %>% filter(str_detect(text,'Trump'))
tidy_body_text_tot <- body_text_tot %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(text != "")
tidy_trump <- tidy_body_text_tot %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_trump %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
#body_text_tot %>% filter(str_detect(text,'Trump'))
tidy_body_text_tot <- body_text_tot %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(str_detect(text,'Trump')) %>%
filter(text != "")
tidy_trump <- tidy_body_text_tot %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_trump %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_trump %>%
with(wordcloud(word, n, max.words = 50))
section_words <- totalarticles %>%
select(webUrl, sectionName) %>%
rename(url = webUrl) %>%
left_join(tidy_body_text_tot, by = "url") %>%
unnest_tokens(word, text) %>%
count(sectionName, word, sort = TRUE)
tot_words <- section_words %>%
group_by(sectionName) %>%
summarize(total = sum(n))
section_words <- left_join(section_words, tot_words, by = "sectionName")
section_tf_idf <- section_words %>%
bind_tf_idf(word, sectionName, n) %>%
arrange(desc(tf_idf))
section_tf_idf %>%
filter(sectionName %in% c("US news", "Global development", "Society", "News", "Politics")) %>%
group_by(sectionName) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = sectionName)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sectionName, ncol = 2, scales = "free") +
labs(x = "tf-idf")
trump_bigrams <- totalarticles %>%
select(webUrl, sectionName) %>%
rename(url = webUrl) %>%
left_join(body_text_tot, by = "url")  %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
filter(!is.na(bigram))
trump_bigram_sep <- trump_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
# Still a graph of positive v.s. negative by section on ratio aspect
# TO DO: same as above.
section_words %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word") %>%
inner_join(get_sentiments("afinn"), by = "word") %>%  # I used "afinn" package here to get the score of each word
group_by(sectionName, word) %>%
summarize(score = n*value) %>%
ungroup() %>%
mutate(sentiment = ifelse(score > 0, "positive", "negative")) %>%
group_by(sectionName, sentiment) %>%
summarize(cum_score = sum(abs(score))) %>%
mutate(ratio = cum_score/sum(cum_score)) %>%
ggplot(aes(x = sectionName, y = ratio, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_brewer(palette = "Set1") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
legend.title = element_blank()) +
labs(title = "The Guardian View on Trump in 2021",
subtitle = "The Ratio of Positive & Negative Score by Section ('afinn')") +
xlab("") +
ylab("Ratio")
# Graph of sentiment score by month with package "afinn".
# might wanna combine guardian score with nytimes score and see the comparison in a single graph
# TO DO: pull all the 2021 articles from nytimes and guardian.
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date) %>%
summarize(tot_score = sum(value)) %>%
filter(date <= "2020-07-01") %>%
ggplot(aes(x = date, y = tot_score, group = 1)) +
geom_line() +
scale_colour_brewer(palette = "Set1") +
scale_x_date(date_breaks="1 month", date_labels="%m-%Y") +
labs(title = "Guardian 2020",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") +
theme_classic()
# Making a graph of positive v.s. negative by section on ratio aspect
# TO DO: try it again after you get all the articles from guardian 2021
section_words %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word") %>%
inner_join(get_sentiments("bing"), by = "word") %>% # I'm using "bing" package here to count the quant of positive and negative words
group_by(sectionName, sentiment) %>%
summarize(quant = sum(n)) %>%
group_by(sectionName) %>%
mutate(ratio = quant/sum(quant)) %>%
ggplot(aes(x = sectionName, y = ratio, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_brewer(palette = "Set1") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
legend.title = element_blank()) +
labs(title = "The Guardian View on Trump in 2021",
subtitle = "The Ratio of Positive & Negative Words by Section ('bing')") +
xlab("") +
ylab("Ratio")
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
load("trump_guardian.RData")
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian.RData")
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/trump_guardian_text.RData")
tidy_body_text_tot <- body_text_tot %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(str_detect(text,'Trump')) %>%
filter(text != "")
section_words <- totalarticles %>%
select(webUrl, sectionName) %>%
rename(url = webUrl) %>%
left_join(tidy_body_text_tot, by = "url") %>%
unnest_tokens(word, text) %>%
count(sectionName, word, sort = TRUE)
section_words %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word") %>%
inner_join(get_sentiments("bing"), by = "word") %>% # I'm using "bing" package here to count the quant of positive and negative words
group_by(sectionName, sentiment) %>%
summarize(quant = sum(n)) %>%
group_by(sectionName) %>%
mutate(ratio = quant/sum(quant)) %>%
ggplot(aes(x = sectionName, y = ratio, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_brewer(palette = "Set1") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
legend.title = element_blank()) +
labs(title = "The Guardian View on Trump in 2021",
subtitle = "The Ratio of Positive & Negative Words by Section ('bing')") +
xlab("") +
ylab("Ratio")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here/'s", "sanders", "joseph", "it/'s", "here's", "jr", "vice", "bernie", "obama", "hampshire", "thursday", "tuesday", "bloomberg", "ms", "gail", "bret", "dr", "buttigieg"))
section_words %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word") %>%
inner_join(get_sentiments("bing"), by = "word") %>% # I'm using "bing" package here to count the quant of positive and negative words
group_by(sectionName, sentiment) %>%
summarize(quant = sum(n)) %>%
group_by(sectionName) %>%
mutate(ratio = quant/sum(quant)) %>%
ggplot(aes(x = sectionName, y = ratio, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_brewer(palette = "Set1") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
legend.title = element_blank()) +
labs(title = "The Guardian View on Trump in 2021",
subtitle = "The Ratio of Positive & Negative Words by Section ('bing')") +
xlab("") +
ylab("Ratio")
section_words %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word") %>%
inner_join(get_sentiments("afinn"), by = "word") %>%  # I used "afinn" package here to get the score of each word
group_by(sectionName, word) %>%
summarize(score = n*value) %>%
ungroup() %>%
mutate(sentiment = ifelse(score > 0, "positive", "negative")) %>%
group_by(sectionName, sentiment) %>%
summarize(cum_score = sum(abs(score))) %>%
mutate(ratio = cum_score/sum(cum_score)) %>%
ggplot(aes(x = sectionName, y = ratio, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_brewer(palette = "Set1") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
legend.title = element_blank()) +
labs(title = "The Guardian View on Trump in 2021",
subtitle = "The Ratio of Positive & Negative Score by Section ('afinn')") +
xlab("") +
ylab("Ratio")
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date) %>%
summarize(tot_score = sum(value)) %>%
filter(date <= "2020-07-01") %>%
ggplot(aes(x = date, y = tot_score, group = 1)) +
geom_line() +
scale_colour_brewer(palette = "Set1") +
scale_x_date(date_breaks="1 month", date_labels="%m-%Y") +
labs(title = "Guardian 2020",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") +
theme_classic()
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date) %>%
summarize(tot_score = sum(value)) %>%
filter(date <= "2021-07-01" && date >= "2021-01-01") %>%
ggplot(aes(x = date, y = tot_score, group = 1)) +
geom_line() +
scale_colour_brewer(palette = "Set1") +
scale_x_date(date_breaks="1 month", date_labels="%m-%Y") +
labs(title = "Guardian 2020",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") +
theme_classic()
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date) %>%
summarize(tot_score = sum(value)) %>%
filter(date <= "2021-07-01") %>%
ggplot(aes(x = date, y = tot_score, group = 1)) +
geom_line() +
scale_colour_brewer(palette = "Set1") +
scale_x_date(date_breaks="1 month", date_labels="%m-%Y") +
labs(title = "Guardian 2020",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") +
theme_classic()
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date) %>%
summarize(tot_score = sum(value)) %>%
filter(date <= "2021-07-01" & date >= "2021-01-01") %>%
ggplot(aes(x = date, y = tot_score, group = 1)) +
geom_line() +
scale_colour_brewer(palette = "Set1") +
scale_x_date(date_breaks="1 month", date_labels="%m-%Y") +
labs(title = "Guardian 2020",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") +
theme_classic()
tidy_body_text_tot %>%
rename(webUrl = url) %>%
left_join(totalarticles, by = "webUrl") %>%
select(webPublicationDate, text) %>%
mutate(date = str_trunc(webPublicationDate, 10, ellipsis = "")) %>%
mutate(date = ymd(date)) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date) %>%
summarize(tot_score = sum(value)) %>%
filter(date <= "2021-07-01" & date >= "2021-01-01") %>%
ggplot(aes(x = date, y = tot_score, group = 1)) +
geom_line() +
scale_colour_brewer(palette = "Set1") +
scale_x_date(date_breaks="1 month", date_labels="%m-%Y") +
labs(title = "Guardian 2021",
subtitle = "Sentiment Score for Trump by Month") +
xlab("") +
ylab("Score") +
theme_classic()
