req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(6)
}
}
dates <- ymd('20200101') + 0:365
d <- format(dates,'%Y%m%d')
totalarticles <- NULL
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(12)
}
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(12)
}
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
key <- "&api-key=ps2dcEy37v0hvMltVu2CGWPqEIhdnbg4"
url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump&begin_date=20200101&end_date=20200101&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
link = "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump"
dates <- ymd('20200101') + 0:365
d <- format(dates,'%Y%m%d')
totalarticles <- NULL
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(12)
}
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
key <- "&api-key=ps2dcEy37v0hvMltVu2CGWPqEIhdnbg4"
url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump&begin_date=20200101&end_date=20200101&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
link = "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump"
dates <- ymd('20200101') + 0:365
d <- format(dates,'%Y%m%d')
totalarticles <- NULL
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(12)
}
}
View(totalarticles)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, error = TRUE)
library(sf)
library(ggplot2)
library(ggthemes) #install.packages("ggthemes")
library(gridExtra) #install.packages('gridExtra')
library(rnaturalearth) #install.packages('rnatrualearth')
library(rnaturalearthdata) #install.packages('rnatrualearthdata')
library(maps)
library(tidyverse)
library(tools)
library(lwgeom)
theme_set(theme_bw())
nyc_sf <- st_read("data/nyc/nyc_acs_tracts.shp")
floridaRivers <- st_read("data/Waterways_Florida")
min <- st_read("data/my_neighborhood_sites")
ggplot(nyc_sf) +
geom_sf() +
theme_map()
plot1 <- ggplot(nyc_sf) +
geom_sf(aes(fill = UNEMP_RATE)) +
labs(title = 'Unemployment Rate in NYC') +
theme_map() + theme(legend.position = "left")
plot1
# can you change the color scheme?
plot2 <- ggplot(nyc_sf) +
geom_sf(aes(fill = poptot )) +
labs(title = 'Population in NYC') +
theme_map() + theme(legend.position = "left")
plot2
grid.arrange(plot1, plot2, ncol = 2)
world <- ne_countries(scale = "medium", returnclass = "sf")
# Basic Map w/ labels
ggplot(data = world) +
geom_sf(color = "black", fill = "#bada55") +
xlab("Longitude") +
ylab("Latitude") +
ggtitle("World Map - Mercator Projection", subtitle = paste0("(", length(unique(world$name)), " countries)")) +
theme_bw()
ggplot(data = world) +
geom_sf() +
coord_sf(crs = "+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs") +
theme_bw() +
ggtitle("Lambert Azimuthal Equal-Area Projection", subtitle = "Correctly represents area but not angles")
ggplot(data = world) +
geom_sf() +
coord_sf(crs = "+proj=fouc") +
theme_bw() +
ggtitle("Foucaut Projection", subtitle = "Correctly represents area, lots of distortion in high latitudes")
ggplot(data = world) +
geom_sf() +
coord_sf(crs = "+proj=natearth2") +
theme_bw() +
ggtitle("Natural Earth II Projection", subtitle = "Represents globe shape, distorted at high latitudes")
ggplot(data = world) +
geom_sf(aes(fill = continent), color = "black") +
coord_sf(crs = "+proj=eqearth") +
theme_bw() +
labs(title = "Equal Earth Projection of World")
ggplot(data = world) +
geom_sf(aes(fill = pop_est)) +
scale_fill_viridis_c(option = "plasma", trans = "log") +
theme_map() +
labs(title = 'World Map', fill = "Population \n Estimate", subtitle = paste0("(", length(unique(world$name)), " countries)"))
sites <- data.frame(longitude = c(-80.1918, -81.3789), latitude = c(25.7617, 28.5384), labs = c("Miami", "Orlando"))
#florida
ggplot(data = world) +
geom_sf() +
geom_point(data = sites, aes(x = longitude, y = latitude), size = 4,
shape = 23, fill = "darkred") +
coord_sf(xlim = c(-88, -78), ylim = c(24.5, 33), expand = FALSE) + #Zoom into this box of the world
geom_text(data = sites, aes(x = longitude, y = latitude + 0.5, label = labs)) +
theme_map()
sites <- data.frame(longitude = c(-80.1918, -81.3789, -81.7842), latitude = c(25.7617, 28.5384, 24.5554), labs = c("Miami", "Orlando", "Key West"))
ggplot(data = world) +
geom_sf() +
geom_point(data = sites, aes(x = longitude, y = latitude), size = 4,
shape = 23, fill = "darkred") +
coord_sf(xlim = c(-88, -78), ylim = c(22, 33), expand = FALSE) + #Zoom into this box of the world
geom_text(data = sites, aes(x = longitude, y = latitude + 0.5, label = labs)) +
theme_map()
# very messy graph that's hard to interpret
# how can you make it better ?
ggplot(data = min) +
geom_sf(aes(color=program_na), alpha = 0.2, size = 0.2) +
coord_sf() +
facet_wrap(vars(program_na))
labs(color = "Program", title = "Minnesota Environmental Concerns by Location, 1980-2023") +
theme_bw()
sf_use_s2(FALSE) #change FALSE to TRUE - what happens?
# hint - check the R documentation. Check your answer w/ the answer key
states <- st_as_sf(maps::map('state', plot = FALSE, fill = TRUE))
states$ID <- stringr::str_to_title(states$ID)
#install.packages('lwgeom') first
states <- cbind(states, st_coordinates(st_centroid(states,of_largest_polygon = TRUE)))
ggplot(data = world) +
geom_sf() +
geom_sf(data = states, aes(fill = ID)) +
geom_text(data = states, aes(X, Y, label = ID), size = 5) +
coord_sf(xlim = c(-88, -78), ylim = c(24.5, 33), expand = FALSE) +
guides(fill = "none")
counties <- st_as_sf(maps::map("county", plot = FALSE, fill = TRUE))
counties <- subset(counties, grepl("florida", counties$ID))
counties$area <- as.numeric(st_area(counties))
# when you've run it once, change the code so that each state is a different color and the boundaries between states are clear.
ggplot(data = world) +
geom_sf(data = counties, fill = '#800020') +
geom_sf(data = floridaRivers, col = 'lightblue') +
scale_fill_viridis_c(trans = "sqrt", alpha = .4) +
coord_sf(xlim = c(-88, -78), ylim = c(24.5, 33), expand = FALSE)
nyc_sf
View(nyc_sf)
View(min)
str(nyc_sf)
View(min)
View(min)
nyc_sf <- st_read("/Users/yiyangshi/Downloads/nyu-2451-34205-shapefile/nyu_2451_34205.shp")
View(nyc_sf)
View(nyc_sf)
View(nyc_sf)
nyc_sf <- st_read("/Users/yiyangshi/Downloads/nynta2020_23a/nynta2020.shp")
View(nyc_sf)
nyc_sf %>% distinct(NTAName)
airbnb <- read.csv("/Users/yiyangshi/Downloads/AB_NYC_2019.csv")
View(airbnb)
airbnb %>% distinct(neighbourhood)
nyc_sf <- nyc_sf %>%
rename(neighborhood = NTAName)
nyc_sf <- nyc_sf %>%
rename(neighbourhood = NTAName)
nyc_sf <- st_read("/Users/yiyangshi/Downloads/nynta2020_23a/nynta2020.shp")
airbnb <- read.csv("/Users/yiyangshi/Downloads/AB_NYC_2019.csv")
nyc_sf <- nyc_sf %>%
rename(neighbourhood = NTAName)
airbnb %>%
left_join(nyc_sf, by = neighbourhood)
airbnb %>%
left_join(nyc_sf, by = "neighbourhood")
airbnb %>%
left_join(nyc_sf, by = "neighbourhood") %>%
na.omit()
load("https://github.com/Tam-Ngn/comp456/blob/main/Articles%20Data/bidenarticle_text.RData")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(ggraph)
data("stop_words")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s"))
load("Articles Data/guardian2021.RData")
load("Articles Data/guardian2021_text.RData")
tidy_body_text_tot <- body_text_tot %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(text != "")
tidy_trump <- tidy_body_text_tot %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_trump %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_trump %>%
with(wordcloud(word, n, max.words = 50))
section_words <- totalarticles %>%
select(webUrl, sectionName) %>%
rename(url = webUrl) %>%
left_join(tidy_body_text_tot, by = "url") %>%
unnest_tokens(word, text) %>%
count(sectionName, word, sort = TRUE)
tot_words <- section_words %>%
group_by(sectionName) %>%
summarize(total = sum(n))
section_words <- left_join(section_words, tot_words, by = "sectionName")
section_tf_idf <- section_words %>%
bind_tf_idf(word, sectionName, n) %>%
arrange(desc(tf_idf))
section_tf_idf %>%
filter(sectionName %in% c("US news", "Global development", "Society", "News", "Politics")) %>%
group_by(sectionName) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = sectionName)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sectionName, ncol = 2, scales = "free") +
labs(x = "tf-idf")
trump_bigrams <- totalarticles %>%
select(webUrl, sectionName) %>%
rename(url = webUrl) %>%
left_join(body_text_tot, by = "url")  %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
filter(!is.na(bigram))
trump_bigram_sep <- trump_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
section_bigram_tf_idf <- trump_bigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
unite(bigram, word1, word2, sep = " ") %>%
count(sectionName, bigram, sort = TRUE)
section_bigram_tf_idf %>%
bind_tf_idf(bigram, sectionName, n) %>%
arrange(desc(tf_idf)) %>%
filter(sectionName %in% c("US news", "Global development", "Society", "News", "Politics")) %>%
group_by(sectionName) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(x = tf_idf, y = fct_reorder(bigram, tf_idf), fill = sectionName)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sectionName, ncol = 2, scales = "free") +
labs(x = "tf-idf")
bigram_graph <- trump_bigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(n > 70) %>%
graph_from_data_frame()
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 3) +
geom_node_text(aes(label = name), vjust = 0.2, hjust = 0.2) +
theme_void()
load("Articles Data/guardian2021.RData")
View(totalarticles)
str_trunc(totalarticles$webPublicationDate, width = 9, ellipsis = "")
str_trunc(totalarticles$webPublicationDate, width = 10, ellipsis = "")
trump_guardian_2021_date<-str_trunc(totalarticles$webPublicationDate, width = 10, ellipsis = "")
trump_guardian_2021_date[1450]
load(guardian2020.RData)
load(/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/Guardian_Trump_Before_API.RData)
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/Guardian_Trump_Before_API.RData")
View(g_t_aug2019_jun2020)
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
key <- "&api-key=9f51672b-1066-4b6f-a6ca-4066419d6f96"
url <- "https://content.guardianapis.com/search?q=trump&from-date=2020-01-01&to-date=2020-01-01&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$results
link = "https://content.guardianapis.com/search?q=trump"
dates <- ymd('20200101') + 0:365
d <- format(dates,'%Y-%m-%d')
totalarticles <- NULL
p = 1
while(p < 10){
url = paste0(link, '&from-date=',i ,'&to-date=',i ,'&page=',p)
req <- try(fromJSON(paste0(url, key)),silent=TRUE)
if(class(req) == 'try-error'){ break }
else{
articles <- req$response$results
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
}
# Sys.sleep(6)
}
for(i in d){
p = 1
while(p < 10){
url = paste0(link, '&from-date=',i ,'&to-date=',i ,'&page=',p)
req <- try(fromJSON(paste0(url, key)),silent=TRUE)
if(class(req) == 'try-error'){ break }
else{
articles <- req$response$results
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
}
# Sys.sleep(6)
}
}
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
key <- "&api-key=9f51672b-1066-4b6f-a6ca-4066419d6f96"
url <- "https://content.guardianapis.com/search?q=trump&from-date=2020-01-01&to-date=2020-01-01&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$results
link = "https://content.guardianapis.com/search?q=trump"
dates <- ymd('20200616') + 0:700
d <- format(dates,'%Y-%m-%d')
totalarticles <- NULL
for(i in d){
p = 1
while(p < 10){
url = paste0(link, '&from-date=',i ,'&to-date=',i ,'&page=',p)
req <- try(fromJSON(paste0(url, key)),silent=TRUE)
if(class(req) == 'try-error'){ break }
else{
articles <- req$response$results
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
}
# Sys.sleep(6)
}
}
View(totalarticles)
dates <- ymd('20200907') + 0:100
dates
dates <- ymd('20200907') + 0:116
dates
dates <- ymd('20210907') + 0:116
dates
d <- format(dates,'%Y-%m-%d')
totalarticles2 <- NULL
for(i in d){
p = 1
while(p < 10){
url = paste0(link, '&from-date=',i ,'&to-date=',i ,'&page=',p)
req <- try(fromJSON(paste0(url, key)),silent=TRUE)
if(class(req) == 'try-error'){ break }
else{
articles <- req$response$results
totalarticles2 <- bind_rows(totalarticles2,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
}
# Sys.sleep(6)
}
}
totalarticles2 <- NULL
totalarticles2 <- NULL
save(totalarticles, file = "trump_guardian_2020.RData")
gc()
load("trump_guardian_2020.RData")
View(totalarticles)
totalarticles2 <- NULL
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
key <- "&api-key=9f51672b-1066-4b6f-a6ca-4066419d6f96"
url <- "https://content.guardianapis.com/search?q=trump&from-date=2020-01-01&to-date=2020-01-01&page=1"
req <- fromJSON(paste0(url, key))
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
key <- "&api-key=9f51672b-1066-4b6f-a6ca-4066419d6f96"
url <- "https://content.guardianapis.com/search?q=trump&from-date=2020-01-01&to-date=2020-01-01&page=1"
req <- fromJSON(paste0(url, key))
totalarticles2 <- NULL
for(i in d){
p = 1
while(p < 10){
url = paste0(link, '&from-date=',i ,'&to-date=',i ,'&page=',p)
req <- try(fromJSON(paste0(url, key)),silent=TRUE)
if(class(req) == 'try-error'){ break }
else{
articles <- req$response$results
totalarticles2 <- bind_rows(totalarticles2,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
}
# Sys.sleep(6)
}
}
key <- "&api-key=9f51672b-1066-4b6f-a6ca-4066419d6f96"
url <- "https://content.guardianapis.com/search?q=trump&from-date=2020-01-01&to-date=2020-01-01&page=1"
req <- fromJSON(paste0(url, key))
key <- "&api-key=395b850e-d5c2-4414-aed5-02beddcbbd23"
url <- "https://content.guardianapis.com/search?q=trump&from-date=2021-09-06&to-date=2021-09-06&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$results
link = "https://content.guardianapis.com/search?q=trump"
dates <- ymd('20210907') + 0:116
d <- format(dates,'%Y-%m-%d')
totalarticles2 <- NULL
for(i in d){
p = 1
while(p < 10){
url = paste0(link, '&from-date=',i ,'&to-date=',i ,'&page=',p)
req <- try(fromJSON(paste0(url, key)),silent=TRUE)
if(class(req) == 'try-error'){ break }
else{
articles <- req$response$results
totalarticles2 <- bind_rows(totalarticles2,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
}
# Sys.sleep(6)
}
}
View(totalarticles2)
totalarticles3 <- rbind(totalarticles, totalarticles2)
View(totalarticles3)
totalarticles <- rbind(totalarticles, totalarticles2)
totalarticles3 <- load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/Guardian_Trump_Before_API.RData")
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/Guardian_Trump_Before_API.RData")
View(g_t_aug2019_jun2020)
totalarticles <- rbind(totalarticles, g_t_aug2019_jun2020)
View(totalarticles)
save(totalarticles, file = "trump_guardian.RData")
load("trump_guardian.RData")
View(totalarticles)
article <- NULL
body_text_tot <- NULL
for (i in 1:length(totalarticles$webUrl)) {
article <- read_html(totalarticles$webUrl[i])
body_text <-
article %>%
html_elements(".dcr-n6w1lc") %>%
html_text()
body_text_coll<- tibble(url = totalarticles$webUrl[i], text = paste(body_text, collapse = " "))
body_text_tot <- bind_rows(body_text_tot, body_text_coll)
}
View(body_text_tot)
