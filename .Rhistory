filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
filter(!word3 %in% stop_words$word) %>%
filter(!word3 %in% mystopwords$word) %>%
count(word1, word2, word3, sort = TRUE) %>%
unite(trigram, word1, word2, word3, sep = " ") %>%
biden_trigram <- tibble(word = paste(bidenarticles$headline$main)) %>%
unnest_tokens(trigram, word, token = "ngrams", n = 3) %>%
filter(!is.na(trigram))
biden_trigram_sep <- biden_trigram %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ")
biden_trigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
filter(!word3 %in% stop_words$word) %>%
filter(!word3 %in% mystopwords$word) %>%
count(word1, word2, word3, sort = TRUE) %>%
unite(trigram, word1, word2, word3, sep = " ")
biden_bigrams <- tibble(word = paste(bidenarticles$abstract,
bidenarticles$snippet,
bidenarticles$lead_paragraph,
bidenarticles$headline$main),
section = bidenarticles$section_name) %>%
unnest_tokens(bigram, word, token = "ngrams", n = 2) %>%
filter(!is.na(bigram))
biden_bigram_sep <- biden_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
section_bigram_tf_idf <- biden_bigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
unite(bigram, word1, word2, sep = " ") %>%
count(section, bigram, sort = TRUE)
section_bigram_tf_idf %>%
bind_tf_idf(bigram, section, n) %>%
arrange(desc(tf_idf)) %>%
filter(section %in% c("U.S.", "Opinion", "World", "Business Day", "Podcasts")) %>%
group_by(section) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(x = tf_idf, y = fct_reorder(bigram, tf_idf), fill = section)) +
geom_col(show.legend = FALSE) +
facet_wrap(~section, ncol = 2, scales = "free") +
labs(x = "tf-idf")
bigram_graph <- biden_bigram_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% mystopwords$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word2 %in% mystopwords$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(n > 70) %>%
graph_from_data_frame()
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 3) +
geom_node_text(aes(label = name), vjust = 0.2, hjust = 0.2) +
theme_void()
section_words <- bidenarticles %>%
select(webUrl, sectionName) %>%
rename(url = webUrl) %>%
left_join(tidy_body_text_tot, by = "url") %>%
unnest_tokens(word, text) %>%
count(sectionName, word, sort = TRUE)
View(bidenarticles)
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
rename(url = web_url) %>%
left_join(tidy_body_text_tot, by = "url") %>%
unnest_tokens(word, text) %>%
count(section_name, word, sort = TRUE)
tidy_body_text_tot <- body_text_tot %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(text != "")
load("guardian2021.RData")
load("guardian2021_text.RData")
tidy_body_text_tot <- body_text_tot %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(text != "")
tidy_trump <- tidy_body_text_tot %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_trump %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
View(tidy_body_text_tot)
load("guardian2021.RData")
load("guardian2021_text.RData")
# Making a graph of positive v.s. negative by section on ratio aspect
bidenarticles %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word") %>%
inner_join(get_sentiments("bing"), by = "word") %>% # I'm using "bing" package here to count the quant of positive and negative words
group_by(sectionName, sentiment) %>%
summarize(quant = sum(n)) %>%
group_by(sectionName) %>%
mutate(ratio = quant/sum(quant)) %>%
ggplot(aes(x = sectionName, y = ratio, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_brewer(palette = "Set1") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
legend.title = element_blank()) +
labs(title = "The Guardian View on Trump in 2021",
subtitle = "The Ratio of Positive & Negative Words by Section ('bing')") +
xlab("") +
ylab("Ratio")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here/'s", "sanders", "joseph", "it/'s", "here's", "jr", "vice", "bernie", "obama", "hampshire", "thursday", "tuesday", "bloomberg"))
```
load("C:/Users/cecei/Desktop/comp465/bidenarticles.RData")
load("C:/Users/cecei/Desktop/comp465/bidenarticlesafter.RData")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here/'s", "sanders", "joseph", "it/'s", "here's", "jr", "vice", "bernie", "obama", "hampshire", "thursday", "tuesday", "bloomberg"))
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(rvest)
library(igraph)
library(tm)
library(ggraph)
data("stop_words")
load("C:/Users/cecei/Desktop/comp465/bidenarticles.RData")
load("C:/Users/cecei/Desktop/comp465/bidenarticlesafter.RData")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here/'s", "sanders", "joseph", "it/'s", "here's", "jr", "vice", "bernie", "obama", "hampshire", "thursday", "tuesday", "bloomberg"))
load("C:/Users/cecei/Desktop/comp465/bidenarticles.RData")
library(rvest)
library(readxl)
full_text <- NULL
for (i in 1:length(bidenarticles$web_url))
{
article <- read_html(bidenarticles$web_url[i])
body_text <-
article %>%
html_elements(".css-at9mc1.evys1bk0") %>%
html_text()
body_text <- data.frame(body_text) %>%
mutate (url = bidenarticles$web_url[i])
full_text <- rbind(data.frame(full_text), body_text)
}
save(full_text, file = "bidenarticle_text.RData")
View(full_text)
View(body_text)
bidenarticles %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word") %>%
inner_join(get_sentiments("bing"), by = "word")
bidenarticles %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word")
View(stop_words)
View(mystopwords)
biden_bigram_sep %>%
filter(word1 == "virus") %>%
count(word1, word2, sort = TRUE)
biden_bigrams <- tibble(word = paste(bidenarticles$abstract,
bidenarticles$snippet,
bidenarticles$lead_paragraph,
bidenarticles$headline$main),
section = bidenarticles$section_name) %>%
unnest_tokens(bigram, word, token = "ngrams", n = 2) %>%
filter(!is.na(bigram))
biden_bigram_sep <- biden_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
biden_bigram_sep %>%
filter(word1 == "virus") %>%
count(word1, word2, sort = TRUE)
biden_bigram_sep %>%
filter(word2 == "virus") %>%
count(word1, word2, sort = TRUE)
virus_words
biden_bigram_sep %>%
filter(word2 == "virus") %>%
count(word1, word2, sort = TRUE)
viruswords <- biden_bigram_sep %>%
filter(words2 == "virus") %>%
inner_join("afinn", by =c(word1 = "word"))
viruswords <- biden_bigram_sep %>%
filter(word2 == "virus") %>%
inner_join("afinn", by =c(word1 = "word"))
viruswords <- biden_bigram_sep %>%
filter(word2 == "virus")
viruswords <- biden_bigram_sep %>%
filter(word2 == "virus") %>%
inner_join(AFINN, by = c(word1, "word"))
viruswords <- biden_bigram_sep %>%
filter(word2 == "virus") %>%
inner_join("AFINN", by = c(word1, "word"))
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(rvest)
library(igraph)
library(tm)
library(ggraph)
data("stop_words")
load("C:/Users/cecei/Desktop/comp465/bidenarticles.RData")
load("C:/Users/cecei/Desktop/comp465/bidenarticlesafter.RData")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here/'s", "sanders", "joseph", "it/'s", "here's", "jr", "vice", "bernie", "obama", "hampshire", "thursday", "tuesday", "bloomberg"))
bidenbefore <- bidenarticles %>%
unnest(headline) %>%
select(abstract, snippet, lead_paragraph, main, pub_date, section_name)
View(bidenbefore)
tidy_body_text_tot <- bidenbefore %>%
select(snippet) %>%
mutate(text = str_remove_all(snippet, "[:punct:]")) %>%
filter(text != "")
View(tidy_body_text_tot)
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
rename(url = web_url) %>%
left_join(tidy_body_text_tot, by = "url") %>%
unnest_tokens(word, text) %>%
count(section_name, word, sort = TRUE)
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
rename(url = web_url)
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
rename(url = web_url)
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
rename(url = web_url) %>%
left_join(tidy_body_text_tot, by = "url")
View(bidenarticles)
bidenbefore <- bidenarticles %>%
unnest(headline) %>%
select(abstract, snippet, lead_paragraph, main, pub_date, section_name, web_url)
tidy_body_text_tot <- bidenbefore %>%
select(snippet) %>%
mutate(text = str_remove_all(snippet, "[:punct:]")) %>%
filter(text != "")
tidy_body_text_tot <- bidenbefore %>%
select(snippet, web_url) %>%
mutate(text = str_remove_all(snippet, "[:punct:]")) %>%
filter(text != "")
tidy_body_text_tot <- bidenbefore %>%
select(snippet, web_url) %>%
mutate(text = str_remove_all(snippet, "[:punct:]")) %>%
filter(text != "")
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
rename(url = web_url) %>%
left_join(tidy_body_text_tot, by = "url") %>%
unnest_tokens(word, text) %>%
count(section_name, word, sort = TRUE)
tidy_body_text_tot <- bidenbefore %>%
select(snippet, web_url) %>%
mutate(text = str_remove_all(snippet, "[:punct:]")) %>%
filter(text != "")
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
rename(url = web_url) %>%
left_join(tidy_body_text_tot, by = "url")
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
rename(url = web_url) %>%
left_join(tidy_body_text_tot, by = "web_url")
tidy_body_text_tot <- bidenbefore %>%
select(snippet, web_url) %>%
mutate(text = str_remove_all(snippet, "[:punct:]")) %>%
filter(text != "")
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
rename(url = web_url) %>%
left_join(tidy_body_text_tot, by = "web_url") %>%
unnest_tokens(word, text) %>%
count(section_name, word, sort = TRUE)
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
left_join(tidy_body_text_tot, by = "web_url")
tidy_body_text_tot <- bidenbefore %>%
select(snippet, web_url) %>%
mutate(text = str_remove_all(snippet, "[:punct:]")) %>%
filter(text != "")
section_words <- bidenarticles %>%
select(web_url, section_name) %>%
left_join(tidy_body_text_tot, by = "web_url") %>%
unnest_tokens(word, text) %>%
count(section_name, word, sort = TRUE)
tot_words <- section_words %>%
group_by(section_name) %>%
summarize(total = sum(n))
section_words <- left_join(section_words, tot_words, by = "section_name")
View(section_words)
knitr::opts_chunk$set(echo = TRUE)
load("guardian2021.RData")
load("guardian2021_text.RData")
tidy_body_text_tot <- body_text_tot %>%
mutate(text = str_remove_all(text, "[:punct:]")) %>%
filter(text != "")
tidy_trump <- tidy_body_text_tot %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
tidy_trump %>%
inner_join(get_sentiments("bing")) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
tidy_biden <- tidy_body_text_tot %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
anti_join(mystopwords) %>%
count(word, sort = TRUE)
View(tidy_biden)
# Making a graph of positive v.s. negative by section on ratio aspect
bidenarticles %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word") %>%
inner_join(get_sentiments("bing"), by = "word") %>% # I'm using "bing" package here to count the quant of positive and negative words
group_by(sectionName, sentiment) %>%
summarize(quant = sum(n)) %>%
group_by(sectionName) %>%
mutate(ratio = quant/sum(quant)) %>%
ggplot(aes(x = sectionName, y = ratio, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_brewer(palette = "Set1") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
legend.title = element_blank()) +
labs(title = "The Guardian View on Trump in 2021",
subtitle = "The Ratio of Positive & Negative Words by Section ('bing')") +
xlab("") +
ylab("Ratio")
bidenarticles %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word")
body_text_tot %>%
anti_join(stop_words, by = "word") %>%
anti_join(mystopwords, by = "word")
View(body_text_tot)
View(tot_words)
View(section_words)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(ggraph)
data("stop_words")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s"))
load("guardian2021.RData")
load("/Users/yiyangshi/Desktop/SPRING 2023/STAT 456/comp465/Articles Data/guardian2021.RData")
load("guardian2021_text.RData")
load("Articles Data/guardian2021.RData")
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph)
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(ggraph)
data("stop_words")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s"))
load("Articles Data/guardian2021.RData")
load("guardian2021_text.RData")
load("Articles Data/guardian2021_text.RData")
load("Article Data/guardian_2020.RData")
load("Article Data/guardian2020.RData")
load("Articles Data/guardian2020.RData")
View(totalarticles)
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
key <- "&api-key=ps2dcEy37v0hvMltVu2CGWPqEIhdnbg4"
url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump&begin_date=20200101&end_date=20200101&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
link = "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump"
dates <- ymd('20200101') + 0:365
d <- format(dates,'%Y%m%d')
totalarticles <- NULL
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(6)
}
}
dates <- ymd('20200101') + 0:365
d <- format(dates,'%Y%m%d')
totalarticles <- NULL
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(12)
}
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(12)
}
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
key <- "&api-key=ps2dcEy37v0hvMltVu2CGWPqEIhdnbg4"
url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump&begin_date=20200101&end_date=20200101&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
link = "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump"
dates <- ymd('20200101') + 0:365
d <- format(dates,'%Y%m%d')
totalarticles <- NULL
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(12)
}
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
key <- "&api-key=ps2dcEy37v0hvMltVu2CGWPqEIhdnbg4"
url <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump&begin_date=20200101&end_date=20200101&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
link = "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=trump"
dates <- ymd('20200101') + 0:365
d <- format(dates,'%Y%m%d')
totalarticles <- NULL
for(i in d){
p = 0
while(p < 10){
url = paste0(link, '&begin_date=',i ,'&end_date=',i ,'&page=',p)
req <- fromJSON(paste0(url, key))
articles <- req$response$docs
totalarticles <- bind_rows(totalarticles,articles)
if(isTRUE(nrow(articles)) && nrow(articles) != 10){ break }
else{p = p+1}
Sys.sleep(12)
}
}
View(totalarticles)
