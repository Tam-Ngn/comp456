# The 2020 Presidential Election: A Sentiment Analysis of News Media Coverage on Joe Biden and Donald Trump

Yiyang Shi, Cecelia Kaufmann, Tam Nguyen

## About our project:

Our project aims to explore the connection between how the two main candidates in the 2020 U.S. presidential election, Joe Biden and Donald Trump were portrayed in the media via sentiment analysis.

To do this, we used the AFINN lexicon to longitudinally explore trends over the span of 2 years, from 2020 to 2022 to see there were connections between specific events in the world or in the candidates lives and how the media portrayed the candidates.

To explore this further, we picked a news source from within the United States, the New York Times and one from the U.K., the Guardian, to see if the trends differ domestically versus internationally. 

## Structure of this repository:

"Pull Request" folder contains two R files: "guardian.R" and "nyt.R". Each of the file includes the pull request code for the articles text, url, headline, abstract, publication date, etc.

"Article Data" contains the RData files that can be directly generated by the code in "Pull Request" folder. We name the RData file in the format of candidate_media_year format.

We put the cleaned data set of articles [here](https://drive.google.com/drive/u/0/folders/11XTb1APraBVzSz_gfn_6YQW6n_BpCELR) in a Google Drive folder, since their sizes are too big to upload on Github. The cleaned data set has one token/word per row with it's corresponding article's url, section name, headline, publication date, etc. Therefore, it's easy to transform them to DTM (Document Term Matrix) format and prepare for the sentiment analysis.

The report of our finding and visualizations is in "FinalReport.Rmd," which includes our research questions, motivations, information about our data sets, time-series visualizations of sentiment scores, and conclusion.

## To reproduce our analysis:

### 1. Data Acquisition 

To pull the articles from NYTimes and Guardian, you will need to register at least one API key from their developer's website ([NYTimes](https://developer.nytimes.com/apis), [Guardian](https://open-platform.theguardian.com/access/)). Both NYTimes and Guardian set a limit on the number of pull requests you can make per day (500 requests/day if using developer key, unlimited requests/day if using commercial key)

If you don't want to spend money on the API key and you still looking for more than 500 requests per day, you will need to register more than one key, and each key should be registered under an unique email address, because NYTimes' daily limit is attached with the email address you registered for the API key. As for the Guardian, the daily limit on the pull request is questionable, meaning you may be able to pull all the articles (from 2020-01-01 to 2022-01-01) in 3 hours, depend on their server's status.

#### (a) NYTimes pull request demonstration (\~/comp456/Pull Request/nyt.R)

To run the code in "nyt.R", you need to install and load the packages:

```{r}
library(jsonlite)
library(tidyverse)
library(lubridate)
library(rvest)
library(httr)
library(dplyr)
library(purrr)
```

After that, you need to put your API key/keys in the list:

```{r}
keys <- c("your key here",
          "your key here",
          ...)
```

Initialize the variables:

```{r}
key <- keys[1]
# you can adjust the keyword after q.
link <- "https://api.nytimes.com/svc/search/v2/articlesearch.json?q=biden" 
# you can adjust the time range of pull request
dates <- ymd('20200101') + 0:730 
# date format for NYTimes only
d <- format(dates,'%Y%m%d')

totalarticles <- NULL
```

Then run the code from line 20 to 62, it will print out the the date, page number, and the number of articles on that page. If it encounters daily limit reached (status code 429), it will jump to the next key. If it has error messages other than 429 and 404, the loop will stop. The articles' urls, headlines, abstract, etc will be stored in "totalarticles" variable. And you will need "totalarticles" to webscrape the article text by running line 74 to 111.

Don't forget to initialize the variables before you run line 74 to 111

```{r}
article <- NULL
body_text_tot <- NULL
failures <- 0
```

The article text and its url will be stored in "body_text_tot". "article" captures the html file generated by each url. "failures" record the number of times our webscrape loop stopped by "too many requests" error messages (status code = 500). We need to pause the loop for a minute or two every time status code 500 pops up, or the status code 500 will show up more frequently, which will low your webscrapping process.

You can store your data by

```{r}
save(totalarticles, file = "candidate_nytimes.RData")
save(body_text_tot, file = "candidate_nytimes_text.RData")
```

#### (b) Guardian pull request demonstration (\~/comp456/Pull Request/guardian.R)

You will need the same set of packages as NYTimes, but this time you may not need that many API keys. Here's how you initialize before you run the pull request code from line 23 to 37:

```{r}
key <- "&api-key=395b850e-d5c2-4414-aed5-02beddcbbd23"
url <- "https://content.guardianapis.com/search?q=trump&from-date=2020-01-01&to-date=2020-01-01&page=1"
req <- fromJSON(paste0(url, key))
articles <- req$response$results
# you can adjust the keyword after q
link = "https://content.guardianapis.com/search?q=trump" 

# you can adjust the time range for the pull request
dates <- ymd('20200101') + 0:365 
d <- format(dates,'%Y-%m-%d')

totalarticles <- NULL
```

Articles' urls, headlines, publication date, etc, will be stored in "totalarticles" same as NYTimes.

Then you will need "totalarticles" to webscrape the article text by running line 44 to 57. You still need to initialize the variable before that:

```{r}
article <- NULL
body_text_tot <- NULL
```

Instead of using status code 429 to stop you webscrapping when you worked on NYTimes, Guardian uses different kinds of CSS selectors to represent their text body. In this way, we put ".article-body-viewer-selector" in the read_html() function, so that it captures all the text bodies no matter what CSS selector Guardian uses.

You can store your data by

```{r}
save(totalarticles, file = "candidate_guardian.RData")
save(body_text_tot, file = "candidate_guardian_text.RData")
```

#### (c) Merging and cleaning

First, we need to load the data

```{r}
# you need to install "stop_words" package 
data("stop_words")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s"))

load("candidate_media.RData")
load("candidate_media_text.RData")
```

then we can merge and prepare the data for the visualizations, here's a demo of merging and cleaning

```{r}
full_trump_guardian <- totalarticles %>% 
  rename(url = webUrl,
         pub_date = webPublicationDate) %>% 
  left_join(body_text_tot, by = "url") %>% 
  select(url, text, pub_date, sectionName) %>% 
  mutate(text = str_remove_all(text, "[:punct:]")) %>% 
  filter(text != "") %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  anti_join(mystopwords) %>% 
  mutate(stem = wordStem(word))
```

You will have a data set that's one word per row, with its url, section name, publication date. We have two data set like this in the [Google Drive folder](https://drive.google.com/drive/u/0/folders/11XTb1APraBVzSz_gfn_6YQW6n_BpCELR), as I mentioned in the "Structure of this repository" section above. You can also download them directly from the folder.

### 2. Visualizations, analysis, and reports (FinalReport.Rmd)

Here are some packages you need to install and load before running the "FinalReport.Rmd" file.

```{r}
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph) 
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(rvest)
library(tm)
library(ggraph)
library(SnowballC)
data("stop_words")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here/'s", "sanders", "joseph", "it/'s", "here's", "jr", "vice", "bernie", "obama", "hampshire", "thursday", "tuesday", "bloomberg", "ms", "gail", "bret", "dr", "buttigieg"))
```

"FinalReport.Rmd" includes the background information about our project, espcially, research questions, motivations, and lexicons, etc. It also has specific introduction on our data sets. It also gives a clear demonstration on data preparation for the analysis, conclusion we reached to, and limitatons of our project.
