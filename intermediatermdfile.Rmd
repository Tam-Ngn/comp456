---
title: "The 2020 Presidental Election: A Sentimen Analysis of the News Media Coverage on Joe Biden and Donald Trump"
author: "Cecelia Kaufmann, Tam Nguyen, Yiyang Shi"
date: '2023-02-28'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion("tufte"))
library(tidyverse)
library(dplyr)
library(lubridate)
library(urltools)
library(scales)
library(textdata)
library(wordcloud)
library(igraph) 
library(ggplot2)
library(tidytext)
library(broom)
library(reshape2)
library(rvest)
library(igraph)
library(tm)
library(ggraph)
data("stop_words")
mystopwords <- tibble(word = c("trump", "trumps", "trump's","trump’s", "biden", "biden's", "biden’s", "donald", "u.s", "joe", "elizabeth", "warren", "here/'s", "sanders", "joseph", "it/'s", "here's", "jr", "vice", "bernie", "obama", "hampshire", "thursday", "tuesday", "bloomberg", "ms", "gail", "bret", "dr", "buttigieg"))
```

# Group Members
The group members are Cecelia Kaufmann, Tam Nguyen, Yiyang Shi

# Area We Are Working In/ Brief Descrption of the Project

We are working in politics, particularly looking at the media portrayal of Donald Trump and Joe Biden leading up to the 2020 election and immediately following (up to a year afterwards). To add nuance, we decided to also look at the difference between the New York Times (a U.S. based/ domestic newspaper) versus the Guardian (a U.K. based/ international newspaper). From there, we plan to use sentiment analysis to analyze both the context of words and the tone of the headlines (positive and negative) in their portrayal of the candidate. 

# Research Question

How Differently Did Domestic Newspapers and International Newspapers Portray Donald Trump and Joe Biden before and after The 2020 U.S. Presidential Election?

Part of why we are interested in looking at both before and after is we hope that this will give a full scope and a plethora of data to analyze using sentiment analysis. We can also look changes in the media portrayal longitudinally when we pull both the pre and post election article data. 

# Description of the Datasets

Here is a breakdown of all the datasets we are using:

##New York Times API 
###Keyword: Trump, Dates: 1-1-2020 through 11-8-2020
###Keyword: Trump, Dates: 1-1-2021 through 1-1-2022
###Keyword: Biden, Dates: 1-1-2020 through 11-8-2020
###Keyword: Biden, Dates: 1-1-2021 through 1-1-2022
The Guardian API
###Keyword: Trump, Dates: 1-1-2020 through 11-8-2020
###Keyword: Trump, Dates: 1-1-2021 through 1-1-2022
###Keyword: Biden, Dates: 1-1-2020 through 11-8-2020
###Keyword: Biden, Dates: 1-1-2021 through 1-1-2022

For both NYT and Guardian, we then pulled the text for all of the articles (though sometimes we could only pull a subset of the articles)


Because we are analyzing news media and their opinion on the presidential candidates, it was important for us to get our hands on articles written about Trump and Biden 1 year leading up to election and 1 year after election day. The way we did it was we used the jsonlite package in R to pull essential information from each newspaper's APIs (NYT and the Guardian). The data we have include article title, url, word count and date of publication. However, APIs did not provide us with full text of each article. Afraid we might not have enough text to work with, we decided to webscrape the full text of the articles using a CSS selector unique to each newspaper.


# New York Times: Biden
## New York Times: Biden Before
```{r}
# Biden Before Articles
load("C:/Users/cecei/Desktop/comp465/Articles Data/bidenarticles.RData")

# Biden Before Text
load("C:/Users/cecei/Desktop/comp465/Articles Data/bidenarticle_text.RData")
```

```{r}
# Selecting and Data Cleaning
bidenbefore <- bidenarticles %>% 
  unnest(headline) %>% 
  select(abstract, snippet, lead_paragraph, main, pub_date, section_name, web_url)

tidy_body_text_tot <- full_text %>%
  mutate(text = str_remove_all(body_text, "[:punct:]")) %>% 
  filter(text != "")


tidy_biden <- tidy_body_text_tot %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  anti_join(mystopwords) %>% 
  count(word, sort = TRUE)
```

```{r}
#with one word
tidy_biden %>% 
  inner_join(get_sentiments("bing")) %>% 
  group_by(sentiment) %>% 
  slice_max(n, n = 10) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  # what is the scale here?
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

SENTENCE HERE

```{r}
section_words <- bidenbefore %>% 
  select(web_url, section_name) %>%
  plyr::rename(c("web_url" = "url")) %>% 
  left_join(tidy_body_text_tot, by = "url") %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  anti_join(mystopwords) %>% 
  count(section_name, word, sort = TRUE)


tot_words <- section_words %>% 
  group_by(section_name) %>% 
  summarize(total = sum(n))

section_words <- left_join(section_words, tot_words, by = "section_name")

section_tf_idf <- section_words %>% 
  bind_tf_idf(word, section_name, n) %>% 
  arrange(desc(tf_idf))

section_tf_idf %>% 
  filter(section_name %in% c("U.S.", "Opinion", "Briefing", "	
Business Day", "World")) %>% 
  group_by(section_name) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>% 
  ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = section_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~section_name, ncol = 2, scales = "free") +
  labs(x = "tf-idf")

```

SENTENCE HERE


```{r}
biden_bigrams <- tibble(word = paste(bidenarticles$abstract, 
                                     bidenarticles$snippet, 
                                     bidenarticles$lead_paragraph, 
                                     bidenarticles$headline$main),
                        section = bidenarticles$section_name) %>% 
  unnest_tokens(bigram, word, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram))

biden_bigram_sep <- biden_bigrams %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

section_bigram_tf_idf <- biden_bigram_sep %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word1 %in% mystopwords$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word2 %in% mystopwords$word) %>% 
  unite(bigram, word1, word2, sep = " ") %>% 
  count(section, bigram, sort = TRUE)

section_bigram_tf_idf %>% 
  bind_tf_idf(bigram, section, n) %>% 
  arrange(desc(tf_idf)) %>% 
  filter(section %in% c("U.S.", "Opinion", "World", "Business Day", "Podcasts")) %>% 
  group_by(section) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>% 
  ggplot(aes(x = tf_idf, y = fct_reorder(bigram, tf_idf), fill = section)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~section, ncol = 2, scales = "free") +
  labs(x = "tf-idf")

bigram_graph <- biden_bigram_sep %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word1 %in% mystopwords$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word2 %in% mystopwords$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 70) %>% 
  graph_from_data_frame()

set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), vjust = 0.2, hjust = 0.2) +
  theme_void()
```

SENTENCE HERE


```{r}
biden_trigram <- tibble(word = paste(bidenarticles$headline$main)) %>%
    unnest_tokens(trigram, word, token = "ngrams", n = 3) %>% 
  filter(!is.na(trigram))

biden_trigram_sep <- biden_trigram %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

biden_trigram_sep %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word1 %in% mystopwords$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word2 %in% mystopwords$word) %>% 
  filter(!word3 %in% stop_words$word) %>% 
  filter(!word3 %in% mystopwords$word) %>% 
  count(word1, word2, word3, sort = TRUE) %>%
  unite(trigram, word1, word2, word3, sep = " ")

```

SENTENCE HERE


```{r}
# Making a graph of positive v.s. negative by section on ratio aspect
section_words %>% 
  anti_join(stop_words, by = "word") %>% 
  anti_join(mystopwords, by = "word") %>% 
  inner_join(get_sentiments("bing"), by = "word") %>% # I'm using "bing" package here to count the quant of positive and negative words
  group_by(sectionName, sentiment) %>% 
  summarize(quant = sum(n)) %>% 
  group_by(sectionName) %>% 
  mutate(ratio = quant/sum(quant)) %>% 
  ggplot(aes(x = sectionName, y = ratio, fill = sentiment)) +
  geom_bar(stat = "identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set1") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.title = element_blank()) +
  labs(title = "The New York Times View on Biden in 2020",
       subtitle = "The Ratio of Positive & Negative Words by Section ('bing')") +
  xlab("") +
  ylab("Ratio")
```
```{r}
section_words %>% 
  anti_join(stop_words, by = "word") %>% 
  anti_join(mystopwords, by = "word") %>% 
  inner_join(get_sentiments("afinn"), by = "word") %>%  # I used "afinn" package here to get the score of each word
  group_by(sectionName, word) %>% 
  summarize(score = n*value) %>% 
  ungroup() %>% 
  mutate(sentiment = ifelse(score > 0, "positive", "negative")) %>% 
  group_by(sectionName, sentiment) %>% 
  summarize(cum_score = sum(abs(score))) %>% 
  mutate(ratio = cum_score/sum(cum_score)) %>% 
  ggplot(aes(x = sectionName, y = ratio, fill = sentiment)) +
  geom_bar(stat = "identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set1") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.title = element_blank()) +
  labs(title = "The New York Times View on Biden in 2020",
       subtitle = "The Ratio of Positive & Negative Score by Section ('afinn')") +
  xlab("") +
  ylab("Ratio")
```

SENTENCES HERE

## New York Times: Biden After


```{r}
## Loading the Data
load("C:/Users/cecei/Desktop/comp465/Articles Data/bidenarticlesafter.RData")

load("C:/Users/cecei/Desktop/comp465/Articles Data/bidenarticlesafter_text.RData")

```


```{r}
bidenafter <- bidenarticlesafter %>% 
  unnest(headline) %>% 
  select(abstract, snippet, lead_paragraph, main, pub_date, section_name, web_url)

tidy_body_text_tot_bidenafter <- text_biden_after %>%
  mutate(text = str_remove_all(body_text, "[:punct:]")) %>% 
  filter(text != "")


tidy_biden <- tidy_body_text_tot %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  anti_join(mystopwords) %>% 
  count(word, sort = TRUE)

#with one word
tidy_biden %>% 
  inner_join(get_sentiments("bing")) %>% 
  group_by(sentiment) %>% 
  slice_max(n, n = 10) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  # what is the scale here?
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```


# New York Times: Trump
## New York Times: Biden Before


## New York Times: Biden After


# The Guardian: Biden
## The Guardian: Biden Before
## The Guardian: Biden After

# The Guardian: Trump
## The Guardian: Trump Before

```{r}
load("Guardian_Trump_Before_API.RData")
```


```{r lowercase and punct removed}
g_t_aug2019_jun2020$webTitle = str_replace_all(g_t_aug2019_jun2020$webTitle, "\\'s|[:punct:]", "")
```



```{r prepare the data}
g_t_section_words <- g_t_aug2019_jun2020 %>%
  select(webTitle,sectionName ) %>% 
  unnest_tokens(word, webTitle) 


g_t_section_words%>%
  filter(!word %in% stop_words$word) %>% 
   pairwise_count(word, sectionName, sort = TRUE)


```


```{r words associated with Trump}
g_t_section_words %>%  
  group_by(word) %>%
  filter(n() > 10) %>% 
  pairwise_cor(word, sectionName, sort = TRUE) %>% 
  filter(!item1 %in% stop_words$word, !item2 %in% stop_words$word) %>% 
  filter(item1 %in% c("trump")) %>% 
  slice_max(correlation, n = 20, with_ties = TRUE) %>% 
  ggplot()+
  geom_col(aes(y = fct_reorder(item2, correlation), x = correlation), fill = "orange")+
  labs(title = "Words Usually Correlated With Trump in The Guardian's Article Titles",
       y = "")+
  theme(plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill = "lightyellow", color = NA),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```

the word "trump" has the highest correlation with the word "world". And on its own, "world" doesn't imply anything other than its literal meaning. so we wondered what words tend to appear with "world"?

```{r words associated with world}
g_t_section_words %>%  
  group_by(word) %>%
  filter(n() > 10) %>% 
  pairwise_cor(word, sectionName, sort = TRUE) %>% 
  filter(!item1 %in% stop_words$word, !item2 %in% stop_words$word) %>% 
  filter(item1 %in% c("world")) %>% 
  slice_max(correlation, n = 20, with_ties = TRUE) %>% 
  ggplot()+
  geom_col(aes(y = fct_reorder(item2, correlation), x = correlation), fill = "orange")+
  labs(title = "Words Usually Correlated With World in The Guardian's Article Titles",
       y = "")+
  theme(plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill = "lightyellow", color = NA),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```

top 2 are "war" and "fight", which carry heavily negative connatations.


Again, we acknowledge the limtations in our method that without specific context, words can have various meanings, depending on our own interpretation. For example, war here can either mean "trade war" or "nuclear war", we however are unable to determine which of these variations is being refered to here.


## The Guardian: Trump After


# Link to Plan for the Rest of the Semester

[Link to our Semester Plan](https://docs.google.com/document/d/1p0PRy2ZPqzWCy_kjIglNMJh0EVDQ5Rl8Hih1olNOW64/edit?usp=sharing)


# Each Group Member's Contribution to the Checkpoint

We all worked on putting in the information for each of the different sources and candidates. For example, 
Cecelia: New York Times, Trump and Biden, Before and After (we already had a lot of the data, so it was repulling and creating visualzations). In Presentation and RMD, the intro, research question, motivations, some of the info about the datasets, etc. 
Yiyang: Guardian, Trump, Before and After
Tam: Guardian, Biden, Before and After










